{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c6bc6f-c794-4870-bdfe-f4ef6fbb2929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 35)\n",
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081d663-130b-40b5-a458-d9de0c75126a",
   "metadata": {},
   "source": [
    "### Parsing `txt` to `parquet` for selected 676 cik and does some cleaning with `pandera`\n",
    "* Casts `cusip` to upper case and cleans empty/zero value or shares\n",
    "* filter outs `cusip` that are non-8 char "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4845c7-4bfd-44da-8145-9e7814a81eb6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# for selected 676 cik, parse txt filings to parquet\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from utils_data_cleaning import end_to_end_parsing\n",
    "\n",
    "selected_cik = [2230,3520,5272,7195,7789,9015,10742,14661,16972,18349,18748,19475,19617,21175,22657,24386,35442,35527,36066,36104,36644,36966,38777,39263,40417,40545,44365,45319,49205,50863,51762,51812,51964,52234,53417,59558,59951,60086,61227,67698,70858,71210,71259,72971,73124,80255,84616,89014,92230,93751,98758,102212,102909,105495,108572,200217,201772,216851,276101,310051,312348,313028,313807,314949,314957,314984,315014,315032,315038,315054,315066,315080,315157,315297,315498,316011,318989,320335,320376,351051,351173,351262,354204,356264,700529,704051,707179,712537,713676,714142,720672,723204,728083,728100,728618,732905,733020,740272,740913,741073,743127,750641,754811,757657,759944,762152,763212,763848,764068,764106,764112,764529,764532,765443,769317,769954,769963,775368,776867,778963,779519,788714,790354,790502,791191,791490,796848,799003,799004,801051,806097,807249,807985,808722,809339,809443,810265,810384,810386,810672,810716,811360,811454,813917,813933,814133,814375,816788,819535,820027,820123,820124,820289,820478,820743,821197,822581,823621,825293,829407,831001,831571,836372,837592,842782,842941,846222,846633,846788,846797,850401,850529,852743,854157,857508,859872,860486,860561,860580,860585,860643,860644,860645,860662,860748,860828,860857,861176,861177,861462,861787,862469,866361,866842,868491,869178,869179,869353,869367,872080,872163,872259,872573,872732,873630,874791,877134,877338,878228,881432,883511,883677,883782,883790,883803,883961,883965,884300,884314,884414,884423,884541,884546,884548,884566,884589,885062,885415,886982,887402,887777,887818,889232,891287,891478,893738,894205,894300,894309,895213,895421,897070,897378,897599,898358,898382,898399,898413,899211,900169,900529,900973,902219,902367,902464,902584,903064,903944,903947,903949,905567,905591,905608,906304,908195,909151,909661,911274,912938,914933,914976,915287,915325,916542,917579,918893,919079,919185,919192,919458,919489,919497,919530,919538,919859,920440,920441,921531,921669,922127,922439,922898,922940,923093,923116,923469,924166,924171,924181,926688,926833,926834,928047,928196,928566,928568,928633,930441,931097,932024,932974,933429,934639,934999,936698,936753,936936,936941,936944,937394,937522,937589,937615,937760,937886,938076,938206,938487,938582,938592,938759,939219,940445,941560,943719,944234,944804,945625,945631,947822,947996,948518,948669,949012,949509,949615,949623,949853,1000097,1000742,1002152,1002672,1002784,1004244,1005354,1005607,1005817,1006364,1006378,1006407,1006435,1007280,1007399,1007524,1008322,1008877,1008894,1008895,1008929,1008937,1009003,1009005,1009012,1009016,1009022,1009076,1009207,1009209,1009232,1009254,1009258,1009262,1010873,1010911,1011443,1011659,1013234,1013536,1013538,1013701,1014306,1014315,1014736,1014738,1015079,1015083,1015086,1015308,1016150,1016287,1016683,1016972,1017115,1017645,1017918,1018331,1018674,1018825,1019231,1020066,1020317,1020580,1020585,1020617,1020918,1021008,1021117,1021223,1021249,1021258,1021642,1021926,1023279,1024716,1025421,1026200,1026710,1027451,1027796,1027817,1029160,1030618,1030815,1031972,1032814,1033225,1033427,1033475,1033505,1033974,1033984,1034184,1034196,1034524,1034541,1034546,1034549,1034642,1034771,1034886,1035350,1035463,1035912,1036248,1036325,1037389,1037558,1037763,1037792,1038661,1039565,1039807,1040190,1040197,1040198,1040210,1040273,1040592,1040762,1041241,1041885,1042046,1044207,1044797,1044905,1044924,1044929,1044936,1046187,1047339,1048921,1049648,1049650,1050442,1050463,1050470,1051359,1052100,1053013,1053054,1053055,1054074,1054425,1054522,1054554,1054677,1055290,1055544,1055963,1055964,1055966,1056053,1056288,1056466,1056488,1056491,1056515,1056516,1056527,1056549,1056559,1056581,1056593,1056807,1056821,1056825,1056827,1056831,1056859,1056958,1056973,1057395,1057439,1058022,1058470,1058800,1059187,1061186,1061768,1062938,1065349,1065350,1066816,1067324,1067926,1067983,1068829,1070134,1071483,1072843,1074027,1074034,1074266,1074273,1076598,1077148,1077583,1078013,1078246,1078658,1078841,1079112,1079114,1079736,1079738,1079930,1080071,1080107,1080117,1080132,1080166,1080171,1080173,1080197,1080201,1080351,1080374,1080380,1080381,1080382,1080386,1080493,1080523,1080628,1080818,1081019,1081198,1082020,1082215,1082327,1082339,1082461,1082491,1082509,1082621,1082917,1083323,1083340,1084207,1084208,1084683,1085041,1085163,1085227,1085601,1085936,1086477,1086483,1086611,1086619,1086762,1086763,1088859,1088875,1088950,1089707,1089755,1089911,1089991,1090413,1091561,1091860,1091923,1092203,1092290,1092351,1092903,1093276,1093589,1094584,1094749,1095836,1096783,1097218,1097278,1097833,1100710,1101250,1102062,1102578,1102598,1103245,1103738,1103804,1103882,1103887,1104186,1104329,1104366,1105468,1105471,1105497,1105837,1105863,1105909,1106129,1106191,1106500,1106505,1106832,1107261,1107310,1108893,1108965,1108969,1109147,1110806,1113629,1114618,1114739,1114928,1115941,1116247,1125727,1125816,1129770,1133219,1134152,1140334,1140771,1142031,1142062,1158583,1389426,1398739,1469219]\n",
    "TR_00_TXT_SEC_APP = Path('/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings')\n",
    "TR_02_TEST_676_CIK_PARQ_SEC_APP = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_TEST_676_CIK_PARQ_SEC_APP')\n",
    "TR_02_FAILURE_CASES_PARQ_SEC_APP = Path(r\"/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_FAILURE_CASES_PARQ_SEC_APP\")\n",
    "\n",
    "filings_parq = TR_02_TEST_676_CIK_PARQ_SEC_APP.glob(\"*.parquet\")\n",
    "f = TR_00_TXT_SEC_APP\n",
    "l = selected_cik\n",
    "subdirs = list(f.glob(\"*\"))\n",
    "subdirs = [d for d in subdirs if d.is_dir() and int(d.name) in l]\n",
    "\n",
    "for subdir in subdirs: # [-20:-1]\n",
    "    filings_txt = list(subdir.glob(\"*.txt\"))\n",
    "    for file in filings_txt:\n",
    "        file_exists = list(TR_02_TEST_676_CIK_PARQ_SEC_APP.glob(f\"{subdir.stem}-{file.stem}*\"))\n",
    "        # Check if the file exists\n",
    "        if len(file_exists) > 0:\n",
    "            continue\n",
    "        else:\n",
    "            # print(\"File does not exist. Converting to parquet...\")\n",
    "            end_to_end_parsing(file, TR_02_TEST_676_CIK_PARQ_SEC_APP, TR_02_FAILURE_CASES_PARQ_SEC_APP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffac596-8947-4040-85a8-5f095166d138",
   "metadata": {},
   "source": [
    "### Parsing `txt` to `parquet` for **ALL** `cik` and does some cleaning with `pandera`\n",
    "* Casts `cusip` to upper case and cleans empty/zero value or shares\n",
    "* filter outs `cusip` that are non-8 char "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf6137-e031-4a88-9769-daf08c417239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parse txt filings to parquet for ALL cik\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from utils_data_cleaning import end_to_end_parsing\n",
    "\n",
    "TR_00_TXT_SEC_APP = Path('/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings')\n",
    "TR_02_ALL_PARQ_SEC_APP = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_ALL_PARQ_SEC_APP')\n",
    "TR_02_FAILURE_CASES_PARQ_SEC_APP = Path(r\"/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_FAILURE_CASES_PARQ_SEC_APP\")\n",
    "\n",
    "# all_filings_parq = TR_02_ALL_PARQ_SEC_APP.glob(\"*.parquet\")\n",
    "subdirs = list(TR_00_TXT_SEC_APP.glob(\"*\"))\n",
    "\n",
    "for subdir in subdirs: # [-20:-1]\n",
    "    filings_txt = list(subdir.glob(\"*.txt\"))\n",
    "    for file in filings_txt:\n",
    "        file_exists = list(TR_02_ALL_PARQ_SEC_APP.glob(f\"{subdir.stem}-{file.stem}*\"))\n",
    "        # Check if the file exists\n",
    "        if len(file_exists) > 0: continue\n",
    "        else: end_to_end_parsing(file, TR_02_ALL_PARQ_SEC_APP, TR_02_FAILURE_CASES_PARQ_SEC_APP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca628589-c88c-4c7a-a4b9-ffbc43cea805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3b50d-cc4d-47d9-b1d7-055f01991f63",
   "metadata": {},
   "source": [
    "### Parse individual `txt` file to `parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdee53-8492-4a1e-a69f-48da57940c58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "from utils_data_cleaning import end_to_end_parsing\n",
    "\n",
    "file = Path(r'/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings/902528/0001178913-20-002231.txt')\n",
    "test_parq = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/test_parq')\n",
    "test_failure = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/test_failure')\n",
    "\n",
    "df_test = end_to_end_parsing(file, test_parq, test_failure)\n",
    "# .sort_values(by='value', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72d39d-7c26-4fbf-b68c-869aac3ba6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_test.head()\n",
    "# df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8327991-8f17-4327-8b23-b6997c1e1701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandera as pa\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "\n",
    "## Function to end-to-end parse the raw `txt` filings, write to a pandas dataframe\n",
    "## and save to a `parquet` file. Only `xml` based filings are dealt with. \n",
    "## The filings prior `xml` format are ignored and instead we use the `csv` filings from dropbox\n",
    "\n",
    "## no division by 1000\n",
    "## cusip transformed to upper case\n",
    "## uses `pandera` to remove `cusip` != length of 9 (as non compliant)\n",
    "## uses `pandera` to remove `values` or `shares` != 0 (as non compliant)\n",
    "## the code writes bad data into a separate dataframes per filings\n",
    "## transformation where we group multiple `cusip` into one only after we have removed \n",
    "# the ones with zeros in `value` or `shares` \n",
    "\n",
    "\n",
    "def end_to_end_parsing(file_path, directory_parquet, failures_parq_dir, data_load_run, cutoff_date = '2023-02-17'):\n",
    "    \"\"\"\n",
    "    func that takes a txt filing's path and parses it to a parquet file\n",
    "    in the `directory_parquet`. It uses `pandera` lazy mode to filter out\n",
    "    bad cusip or empty/nan/zero shares or values. Bad records are written\n",
    "    to the `failures_parq_dir` for a later analysis or correction.\n",
    "    \n",
    "    \"\"\"\n",
    "    def work_content_extract(file_path):\n",
    "        \"\"\"extracting 1. xml or not flag, 2. xml part of filings, SEC header part\"\"\"\n",
    "        open_file = open(file_path, \"r\")\n",
    "        file_content = open_file.read()\n",
    "        open_file.close()\n",
    "\n",
    "        # find block of text between two words/tags/strings\n",
    "        start_header = \"<SEC-HEADER>\"\n",
    "        end_header = \"</SEC-HEADER>\"\n",
    "        block_header_search = re.compile(f\"{start_header}.*?{end_header}\", re.DOTALL)\n",
    "        pages_header = re.findall(block_header_search, file_content)\n",
    "\n",
    "        # check if file is XML or not\n",
    "        xml_tag_search = re.compile(r\"<xml>\", flags=re.IGNORECASE)\n",
    "        if xml_tag_search.search(file_content):\n",
    "            xml_flag = \"YES\"\n",
    "\n",
    "            # xml block search\n",
    "            start_xml = \"<XML\\>\"\n",
    "            end_xml = \"<\\/XML\\>\"\n",
    "            block_xml_search = re.compile(f\"{start_xml}(.*?){end_xml}\", re.DOTALL)\n",
    "            pages_xml = re.findall(block_xml_search, file_content)\n",
    "\n",
    "            if len(pages_xml) == 0:\n",
    "                reportType = \"\"\n",
    "                document0 = \"\"\n",
    "                document1 = \"\"\n",
    "            elif len(pages_xml) == 1:\n",
    "                document0 = bs(pages_xml[0], \"xml\")\n",
    "                coverPage = document0.find(\"coverPage\")\n",
    "                reportType = coverPage.find(\"reportType\").text.strip()\n",
    "                document1 = \"\"\n",
    "            if len(pages_xml) > 1:\n",
    "                document0 = bs(pages_xml[0], \"xml\")\n",
    "                coverPage = document0.find(\"coverPage\")\n",
    "                reportType = coverPage.find(\"reportType\").text.strip()\n",
    "                document1 = bs(pages_xml[1], \"xml\")\n",
    "\n",
    "        else:\n",
    "            xml_flag = \"NO\"\n",
    "            pages_xml = []\n",
    "            reportType = \"\"\n",
    "            document1 = \"\"\n",
    "\n",
    "        return pages_header, pages_xml, xml_flag, reportType, document1\n",
    "\n",
    "    def parse_institutionalInvestorInfo(file_path):\n",
    "        dataDictionary = dict()\n",
    "        dataDictionary[\"edgar_path\"] = file_path\n",
    "        periodOfReport = datetime.strptime(\n",
    "            re.compile(r\"(?<=CONFORMED PERIOD OF REPORT:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip(),\n",
    "            \"%Y%m%d\",\n",
    "        ).date()\n",
    "        report_Year, report_Quarter = (\n",
    "            periodOfReport.year,\n",
    "            (periodOfReport.month - 1) // 3 + 1,\n",
    "        )\n",
    "\n",
    "        dataDictionary[\"accessionNumber\"] = (\n",
    "            re.compile(r\"(?<=ACCESSION NUMBER:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"cikManager\"] = (\n",
    "            re.compile(r\"(?<=CENTRAL INDEX KEY:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"managerName\"] = (\n",
    "            re.compile(r\"(?<=COMPANY CONFORMED NAME:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"periodOfReport\"] = periodOfReport\n",
    "        dataDictionary[\"report_Quarter\"] = report_Quarter\n",
    "        dataDictionary[\"report_Year\"] = report_Year\n",
    "        dataDictionary[\"submissionType\"] = (\n",
    "            re.compile(r\"(?<=CONFORMED SUBMISSION TYPE:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"filedAsOfDate\"] = datetime.strptime(\n",
    "            re.compile(r\"(?<=FILED AS OF DATE:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip(),\n",
    "            \"%Y%m%d\",\n",
    "        ).date()\n",
    "        dataDictionary[\"xml_flag\"] = xml_flag\n",
    "        dataDictionary[\"created_at\"] = datetime.now()\n",
    "        dataDictionary[\"updated_at\"] = datetime.now()\n",
    "\n",
    "        # dataDictionary = dict()\n",
    "        dataDictionary[\"edgar_path\"] = file_path\n",
    "        if xml_flag == \"NO\":\n",
    "\n",
    "            dataDictionary[\"isAmendment\"] = \"\"\n",
    "            dataDictionary[\"amendmentType\"] = \"\"\n",
    "            dataDictionary[\"entryTotal\"] = int(\"0\" + \"\")\n",
    "            dataDictionary[\"valueTotal\"] = float(\"0.0\" + \"\")\n",
    "            dt = pd.DataFrame.from_dict([dataDictionary])\n",
    "\n",
    "        else:\n",
    "            if len(pages_xml) == 0:\n",
    "                dataDictionary[\"isAmendment\"] = \"\"\n",
    "                dataDictionary[\"amendmentType\"] = \"\"\n",
    "                dataDictionary[\"entryTotal\"] = int(\"0\" + \"\")\n",
    "                dataDictionary[\"valueTotal\"] = float(\"0.0\" + \"\")\n",
    "                dt = pd.DataFrame.from_dict([dataDictionary])\n",
    "\n",
    "            else:\n",
    "                document = bs(pages_xml[0], \"xml\")\n",
    "                # get sections\n",
    "                coverPage = document.find(\"coverPage\")\n",
    "                summaryPage = document.find(\"summaryPage\")\n",
    "                # get data\n",
    "                if coverPage.find(\"isAmendment\") is not None:\n",
    "\n",
    "                    if coverPage.find(\"isAmendment\").text.strip() == \"true\":\n",
    "                        dataDictionary[\"isAmendment\"] = coverPage.find(\n",
    "                            \"isAmendment\"\n",
    "                        ).text.strip()\n",
    "                        if coverPage.find(\"amendmentType\"):\n",
    "                            dataDictionary[\"amendmentType\"] = coverPage.find(\n",
    "                                \"amendmentType\"\n",
    "                            ).text.strip()\n",
    "                        else:\n",
    "                            dataDictionary[\"amendmentType\"] = \"\"\n",
    "                    else:\n",
    "                        dataDictionary[\"isAmendment\"] = coverPage.find(\n",
    "                            \"isAmendment\"\n",
    "                        ).text.strip()\n",
    "                        dataDictionary[\"amendmentType\"] = \"\"\n",
    "                else:\n",
    "                    dataDictionary[\"isAmendment\"] = \"\"\n",
    "                    dataDictionary[\"amendmentType\"] = \"\"\n",
    "\n",
    "                if summaryPage is not None:\n",
    "                    if summaryPage.find(\"tableEntryTotal\").text.strip():\n",
    "                        dataDictionary[\"entryTotal\"] = int(\n",
    "                            float(summaryPage.find(\"tableEntryTotal\").text.strip() + \"0.0\")\n",
    "                        )\n",
    "                    else:\n",
    "                        dataDictionary[\"entryTotal\"] = int(\"0\" + \"\")\n",
    "\n",
    "\n",
    "                    if summaryPage.find(\"tableValueTotal\").text.strip():\n",
    "                        dataDictionary[\"valueTotal\"] = float(\n",
    "                        summaryPage.find(\"tableValueTotal\").text.strip()\n",
    "                    + \"0.0\")\n",
    "\n",
    "                    else: dataDictionary[\"valueTotal\"] = float(\"0.0\" + \"\")\n",
    "                else:\n",
    "                    dataDictionary[\"entryTotal\"], dataDictionary[\"valueTotal\"] = int(\n",
    "                        \"0\" + \"\"\n",
    "                    ), float(\"0.0\" + \"\")\n",
    "\n",
    "                # create dataframe\n",
    "                dt = pd.DataFrame.from_dict([dataDictionary])\n",
    "\n",
    "        return dt\n",
    "\n",
    "    def parse_institutionalInvestorPortfolio(file_path):\n",
    "        check = re.compile(\"13F HOLDINGS REPORT|13F COMBINATION REPORT\")\n",
    "        if check.search(reportType) is not None and len(pages_xml) > 1:\n",
    "\n",
    "            portfolio = list()\n",
    "            # find all securities held\n",
    "\n",
    "            securities = document1.find_all(\"infoTable\")\n",
    "            for row in securities:\n",
    "                portfolioDict = dict()\n",
    "                portfolioDict[\"edgar_path\"] = file_path\n",
    "                portfolioDict[\"cusip\"] = row.find(\"cusip\").text.strip()\n",
    "                portfolioDict[\"nameOfIssuer\"] = row.find(\"nameOfIssuer\").text.strip()\n",
    "                portfolioDict[\"titleOfClass\"] = row.find(\"titleOfClass\").text.strip()\n",
    "                portfolioDict[\"sharesValue\"] = float(\n",
    "                    row.find(\"value\").text.strip()\n",
    "                )\n",
    "                portfolioDict[\"sharesHeldAtEndOfQtr\"] = int(\n",
    "                    float(row.find(\"sshPrnamt\").text.strip())\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    portfolioDict[\"sharePriceAtEndOfQtr\"] = round(\n",
    "                        portfolioDict[\"sharesValue\"]\n",
    "                        / portfolioDict[\"sharesHeldAtEndOfQtr\"],\n",
    "                        2,\n",
    "                    )\n",
    "                except ZeroDivisionError:\n",
    "                    portfolioDict[\"sharePriceAtEndOfQtr\"] = float(int(\"0\" + \"\"))\n",
    "\n",
    "                portfolioDict[\"shares_bonds\"] = row.find(\"sshPrnamtType\").text.strip()\n",
    "\n",
    "                if row.find(\"putCall\") is not None:\n",
    "                    portfolioDict[\"putCall\"] = row.find(\"putCall\").text.strip()\n",
    "                else:\n",
    "                    portfolioDict[\"putCall\"] = \"\"\n",
    "\n",
    "                portfolio.append(pd.DataFrame.from_dict([portfolioDict]))\n",
    "\n",
    "            # concatanate secutires\n",
    "            dtPortfolio = pd.concat(portfolio, sort=False, ignore_index=True)\n",
    "        else:\n",
    "            dtPortfolio = pd.DataFrame.from_dict(\n",
    "                [\n",
    "                    {\n",
    "                        \"cusip\": \"\",\n",
    "                        \"nameOfIssuer\": \"\",\n",
    "                        \"titleOfClass\": \"\",\n",
    "                        \"sharesValue\": float(\"0.0\" + \"\"),\n",
    "                        \"sharesHeldAtEndOfQtr\": int(\"0\" + \"\"),\n",
    "                        \"sharePriceAtEndOfQtr\": float(\"0.0\" + \"\"),\n",
    "                        \"shares_bonds\": \"\",\n",
    "                        \"putCall\": \"\",\n",
    "                        \"edgar_path\": file_path,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return dtPortfolio\n",
    "\n",
    "    pages_header, pages_xml, xml_flag, reportType, document1 = work_content_extract(\n",
    "        file_path\n",
    "    )\n",
    "    df1, df2 = parse_institutionalInvestorInfo(\n",
    "        file_path\n",
    "    ), parse_institutionalInvestorPortfolio(file_path)\n",
    "\n",
    "    data = pd.merge(df1, df2, on=\"edgar_path\")\n",
    "    data = data.assign(dsource='sec_app')    \n",
    "    data = data.rename(columns={'accessionNumber': 'accession_number', \"cikManager\": 'cik', \"managerName\":'cik_name','xml_flag':'type',\n",
    "           \"periodOfReport\": 'rdate', \"submissionType\": 'submission_type', \"filedAsOfDate\":'fdate', \"entryTotal\": \"entry_total\",\n",
    "           \"valueTotal\": \"value_total\", 'putCall':'put_call', \"sharesValue\":'value', \"sharesHeldAtEndOfQtr\":'shares',\n",
    "           \"securityType\": \"security_type\", 'titleOfClass':'title_of_class', 'nameOfIssuer':'name_of_issuer' , \"edgar_path\":'file'})\n",
    "\n",
    "    column_names = [\n",
    "        \"accession_number\",\n",
    "        \"cik\",\n",
    "        \"cik_name\",\n",
    "        \"rdate\",\n",
    "        \"submission_type\",\n",
    "        \"fdate\",\n",
    "        \"entry_total\",\n",
    "        \"value_total\",\n",
    "        \"cusip\",\n",
    "        \"name_of_issuer\",\n",
    "        \"title_of_class\",\n",
    "        \"value\",\n",
    "        \"shares\",\n",
    "        \"shares_bonds\",\n",
    "        \"put_call\",\n",
    "        \"type\",\n",
    "        \"file\",\n",
    "        'dsource',\n",
    "        \"created_at\",\n",
    "        'updated_at',\n",
    "        'data_load_run'\n",
    "    ]\n",
    "    data = data.astype(\n",
    "        {\n",
    "            \"accession_number\": str,\n",
    "            \"cik\": \"Int64\",\n",
    "            'cik_name': str,\n",
    "            \"rdate\": \"datetime64[ns]\",\n",
    "            \"submission_type\": str,\n",
    "            \"fdate\": \"datetime64[ns]\",\n",
    "            \"entry_total\": \"Int64\",\n",
    "            \"value_total\": \"float64\",\n",
    "            \"cusip\": str,\n",
    "            \"name_of_issuer\": str,\n",
    "            \"title_of_class\": str,\n",
    "            \"value\": \"float64\",\n",
    "            \"shares\": \"float64\",\n",
    "            \"shares_bonds\": str,\n",
    "            \"put_call\": str,\n",
    "            \"type\": str,\n",
    "            \"file\": str,\n",
    "            'dsource': str,\n",
    "            \"created_at\": \"datetime64[ns]\",\n",
    "            \"updated_at\": \"datetime64[ns]\"\n",
    "        }\n",
    "    )\n",
    "    attributes = {\n",
    "        \"cik\": \"first\",\n",
    "        \"cik_name\": \"first\",\n",
    "        \"rdate\": \"first\",       \n",
    "        \"fdate\": \"first\",\n",
    "        \"cusip\": \"first\",\n",
    "        \"name_of_issuer\": \"first\",        \n",
    "        \"value\": \"sum\",\n",
    "        \"shares\": \"sum\",\n",
    "        \"title_of_class\": \"first\",\n",
    "        \"shares_bonds\": \"first\",\n",
    "        \"put_call\": \"first\",\n",
    "        \"accession_number\": \"first\",\n",
    "        \"submission_type\": \"first\",\n",
    "        \"type\": \"first\",\n",
    "        \"file\": \"first\",\n",
    "        'dsource': 'first',\n",
    "        \"entry_total\": \"first\",\n",
    "        \"value_total\": \"first\",\n",
    "        \"created_at\": \"first\",\n",
    "        \"updated_at\": \"first\",\n",
    "        'data_load_run': 'first'\n",
    "    }\n",
    "    data = data.assign(cusip=data.cusip.str.upper(),\n",
    "                       data_load_run=data_load_run)\n",
    "    data = data.reindex(columns=column_names)\n",
    "   \n",
    "    ## pandera code\n",
    "    validation_schema = pa.DataFrameSchema({\n",
    "    \"cusip\": pa.Column(str,\n",
    "                       pa.Check(lambda s: s.str.len() == 9),\n",
    "                       required=True, nullable=False),\n",
    "    \"value\":  pa.Column(float, pa.Check(lambda s: s != 0.0), required=True, nullable=False),\n",
    "    \"shares\": pa.Column(float, pa.Check(lambda s: s != 0.0), required=True, nullable=False)\n",
    "        })\n",
    "    if data.head(1).type.squeeze() != 'NO' and data.head(1).fdate.squeeze() > pd.to_datetime(cutoff_date,format='%Y-%m-%d'):\n",
    "        try:\n",
    "            validation_schema.validate(data, lazy=True)\n",
    "            if not data.empty:\n",
    "                # group cusips\n",
    "                df2 = data.groupby([\"cusip\"], as_index=False).agg(attributes)\n",
    "                df2.to_parquet(\n",
    "                    os.path.join(\n",
    "                        directory_parquet,\n",
    "                        f\"{df2.head(1).cik[0]}-{df2.head(1).accession_number[0]}-{df2.head(1).fdate[0].strftime('%Y-%m-%d')}.parquet\",\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                # return df2\n",
    "            else: pd.DataFrame()\n",
    "\n",
    "        except pa.errors.SchemaErrors as e:\n",
    "            failure_cases = e.failure_cases\n",
    "            failure_cases = (failure_cases.assign(df_file=data.file,\n",
    "                                                 df_cik=data.cik,\n",
    "                                                 df_accession_number=data.accession_number,\n",
    "                                                 df_rdate=data.rdate,\n",
    "                                                 df_fdate=data.fdate,\n",
    "                                                 df_value=data.value)).astype({'failure_case':str})\n",
    "            # print(failure_cases)\n",
    "            failure_cases.to_parquet(Path.joinpath(failures_parq_dir, \\\n",
    "                                                   f\"bad-{data.head(1).cik[0]}-{data.head(1).accession_number[0]}-{data.head(1).fdate[0].strftime('%Y-%m-%d')}.parquet\"))\n",
    "            data = data[~data.index.isin(failure_cases[\"index\"])]\n",
    "            \n",
    "            if not data.empty:\n",
    "                # group cusips\n",
    "                df2 = data.groupby([\"cusip\"], as_index=False).agg(attributes)\n",
    "                df2.to_parquet(\n",
    "                    os.path.join(\n",
    "                        directory_parquet,\n",
    "                        f\"{df2.head(1).cik[0]}-{df2.head(1).accession_number[0]}-{df2.head(1).fdate[0].strftime('%Y-%m-%d')}.parquet\"))\n",
    "                \n",
    "\n",
    "            else: pd.DataFrame()\n",
    "        if 'cik' and 'accession_number' in df2.columns: return {'data_load_run':data_load_run,'cik':df2.head(1).cik[0], 'accession_number': df2.head(1).accession_number[0], 'qa':'no'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c63c78-8e51-48da-b05b-cce99d46766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read list of csvs with variable number of columns\n",
    "# we define what columns we need \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# txt = \"\"\"AB,CD,EF,GH\n",
    "# foo,20160101,a,1\n",
    "# foo,20160102,a,3\n",
    "# foo,20160103,a,5\"\"\"\n",
    "\n",
    "txt2 = \"\"\"AB,CD,EF\n",
    "foo,20160101,1\n",
    "foo,20160102,1\n",
    "foo,20160103,1\"\"\"\n",
    "\n",
    "# usecols = ['AB', 'CD', 'EF', 'GH','IJ']\n",
    "\n",
    "# df = pd.read_csv(StringIO(txt2), usecols=lambda c: c in set(usecols))\n",
    "df = pd.read_csv(StringIO(txt2))\n",
    "df.EF.head(1).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af6ae9-aa9b-4807-a6b3-aeb76843d123",
   "metadata": {},
   "source": [
    "#### A very interesting use of lambda, dictionary comprehension creacion from Andrej Karpathy's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cbc8ed-1c04-420e-92de-1be7cc20eed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.ascii_letters\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(string.ascii_letters+' ')}\n",
    "itos = {i:ch for i, ch in enumerate(string.ascii_letters+' ')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efe9e9-e093-4bb1-bc8b-2bfa3e640a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stoi\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "encode('hii itheerre')\n",
    "decode(encode('hii itheerre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423bda3-9666-4019-8a9d-94f10fb4f9a7",
   "metadata": {},
   "source": [
    "#### Let's practice a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a1363-46e8-49ba-8905-8a8d9c68b206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lambda func to list all files in a given dir\n",
    "from pathlib import Path\n",
    "sec_app_parq = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_ALL_PARQ_SEC_APP')\n",
    "\n",
    "# list_files_in_dir = lambda dir_: sorted(list(Path(dir_).glob('*'))) \n",
    "# list_cik_in_dir = lambda dir_: sorted(set([int(file.stem.split('-')[0]) for file in Path(dir_).glob('*')]))\n",
    "list_fdates_in_dir = lambda dir_: sorted(set([file.stem.split('-', 4)[-1] for file in Path(dir_).glob('*')]))\n",
    "# list_cik_after = lambda dir_, cik_after: [cik for cik in list_cik_in_dir(dir_) if cik > cik_after ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc9d5d-737d-4d89-afd8-b345b9de406f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# list_files(sec_app_parq)[:4]\n",
    "# list_cik_in_dir(sec_app_parq)[:4]\n",
    "# list_fdates_in_dir(sec_app_parq)[:4]\n",
    "\n",
    "min(list_fdates_in_dir(sec_app_parq)), max(list_fdates_in_dir(sec_app_parq))\n",
    "\n",
    "# print(itemgetter(0, -1)(list_cik_after(sec_app_parq, 1958491)))\n",
    "\n",
    "# list_cik_in_dir = lambda dir_: sorted(set([int(file.stem.split('-')[0]) for file in Path(dir_).glob('*')]))\n",
    "# list_cik_after = lambda dir_, cik_after: [cik for cik in list_cik_in_dir(dir_) if cik > cik_after ]\n",
    "# list_cik_after(sec_app_parq, 1958491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878240a-ff4b-40d6-bf7b-9755e40bed26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b1450-77a0-4846-a3de-1a9a5f5379e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getattr(object, 'y', 5)\n",
    "# getattr(object, 'x') is completely equivalent to object.x , but getattr(object, 'y', 5), sets a defaut value and won't error out if 'y' doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885691ae-a291-4b27-b514-08b83bc71ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "l = ['2013-05-20', '2013-05-21', '2013-05-22', '2013-05-23', '2017-05-23']\n",
    "\n",
    "itemgetter(1, 2)(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd8903-fe08-40e3-b5ef-47f903dd0750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "falseValue = 345\n",
    "trueValue  = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90647cda-9c8a-4252-82b2-6f97f5fa445b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(falseValue, trueValue)[bool(True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90173255-781f-4585-b9fb-eabbd846981a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d8af8-04ad-4af8-a7eb-0519b1cb976f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(lambda: print('I was wrong'), lambda: print('I was right'))[bool(True)]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eec6b5-0792-4f06-9342-d6828e3a0d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = print('I was right') if True else rint('I was wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae184a0-189b-4df3-abe7-c49377475620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ternary operator\n",
    "for date in l:\n",
    "    x = 'dropbox' if date <= '2014-01-01' else 'sec_app'\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b57f0-7000-40e0-b198-99cf6c11048a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# another way how to launch programs from python\n",
    "import subprocess\n",
    "file = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_ALL_PARQ_SEC_APP/1953154-0001214659-22-013465-2022-11-10.parquet')\n",
    "# subprocess.call([\"ls\", \"-l\"])\n",
    "subprocess.call(['tad', file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba042a-3d66-46c3-866d-dc097175eb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549f233-bc1b-4b5a-a894-db2cfe7683e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda func to list all files in a given dir\n",
    "from pathlib import Path\n",
    "sec_app_parq = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_ALL_PARQ_SEC_APP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bfd57-0475-4735-b49c-36d8182f4f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_file_names = lambda _dir: [file.stem for file in Path(_dir).glob('*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dc3b9-9baa-4302-8a9b-70387ea26c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_file_names(sec_app_parq)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad56904-b338-40ee-b77a-0a15050448ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "last_date = lambda _dir: [file.stem.split('-', 4)[-1] for file in Path(_dir).glob('*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba6882-fa4e-4b00-9fb7-ba634175dbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min(last_date(sec_app_parq)), max(last_date(sec_app_parq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03edc7b3-a6e9-4c70-9ccf-a533caa59356",
   "metadata": {},
   "source": [
    "#### Lambda with two parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63897d4e-a388-4bc4-ba9c-f19d2747e612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_parq_in_dir = lambda folder, ext: [file.stem for file in Path(folder).glob('*') if file.suffix == ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099d9f4-3179-4915-91e7-958400bc0767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_parq_in_dir(sec_app_parq, '.csv')[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b646a4-2394-4a91-8644-f6b1704cea00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'865623-0000922423-01-500513-2001-07-16'.rsplit('-', 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9231f-83bf-4049-b6b1-fd032b7507f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "list_file_names_csv = lambda folder: [file.stem for file in folder.glob('*.csv')]\n",
    "list_file_names_parq = lambda folder: [file.stem.rsplit('-', 3)[0] for file in folder.glob('*.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8c0e3-f345-43ec-8f80-4c3891f9a03c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_csv_0_2013 = Path(r\"/Users/yo_macbook/Documents/app_data/dropbox_13f_files/processed_tables/TR_01_ALL_CSV_CLEANED\")\n",
    "clean_parq_0_2013 = Path(r'/Users/yo_macbook/Documents/app_data/dropbox_13f_files/processed_tables/TR_02_CLEANED_PARQ_0_2013')\n",
    "\n",
    "csv_files = list_file_names_csv(clean_csv_0_2013)\n",
    "parquet_files = list_file_names_parq(clean_parq_0_2013)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8cad1-6595-423b-8b0b-7d39eb30fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaf8da-e8e7-4bbf-9e0b-1a15324b0ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(csv_files), len(parquet_files) \n",
    "csv_files[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a4c69-e68b-4be8-b465-d3dab685d74f",
   "metadata": {},
   "source": [
    "### Find missing parquet files. It's difference between csv and parquet lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb918d72-02c6-4b48-b7e7-2c27e8bc703c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l = np.setdiff1d(csv_files, parquet_files)\n",
    "l_reverse = np.setdiff1d(parquet_files, csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7430e-3337-41e9-be76-de24254f2fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[print(name) for name in l if name not in parquet_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2f61d-1e33-459a-9a8e-bbf4bc552375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path.joinpath(clean_csv_0_2013, '1215613-0000905718-03-000066.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201e8b1-bd3e-49d4-b294-98c5593ebb16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_files = lambda folder: [file.stem for file in folder.iterdir()]\n",
    "\n",
    "clean_parq_0_2013 = Path(r'/Users/yo_macbook/Documents/app_data/dropbox_13f_files/processed_tables/TR_02_CLEANED_PARQ_0_2013')\n",
    "clean_parq_2014_2023 = Path('/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_FINAL_PARQ_2014_2023_02_17')\n",
    "\n",
    "final_parq = Path(f'/Users/yo_macbook/Documents/app_data/TR_02_FINAL_PARQ_BOTH_1999_2023_02_17')\n",
    "print(f'first part: {len(list_files(clean_parq_0_2013))} + second part: {len(list_files(clean_parq_2014_2023))} = {len(list_files(clean_parq_0_2013)) + len(list_files(clean_parq_2014_2023))}')\n",
    "print(f'final folder: {len(list_files(final_parq))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d790bfe-6aaf-461e-8cb1-a764949698a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "common_files = np.intersect1d(list_files(clean_parq_0_2013), list_files(clean_parq_2014_2023) )\n",
    "print(len(common_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7dad8-c7b0-4a42-aa1c-c2c9057a1b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_files(clean_parq_0_2013)[:3], list_files(clean_parq_2014_2023)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c3a88-77fa-444a-b424-54ed63b4ced5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "len(list_files(final_parq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7e849-7917-4ab0-ac21-e8f4a5271459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "common_elements = list(set(list_files(clean_parq_0_2013)) & set(list_files(clean_parq_2014_2023)))\n",
    "\n",
    "print(len(common_elements) if common_elements else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fb544-aeb8-453c-8028-374f60f21e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "642/482"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0edd33e-bd16-4df4-b087-28ba58d09ac7",
   "metadata": {},
   "source": [
    "### Copying file from one folder to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a3728b-6c4d-4aad-b2ac-b93caca58ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "copy_files = lambda folder_from, folder_to: [shutil.copy(file, folder_to) for file in folder_from.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d504bd0-5aee-4934-8182-23e571f9a3a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_files(clean_parq_2014_2023, final_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162312b-8af9-4797-8a3f-28f5c0e742d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "temp_parq = Path(f'/Users/yo_macbook/Documents/app_data/TR_02_FINAL_PARQ_BOTH_1999_2023_02_17')\n",
    "final_parq = Path(r'/Users/yo_macbook/Documents/app_data/TR_02_PARQ_ALL')\n",
    "\n",
    "copy_files(temp_parq, final_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a607e-d021-49e2-93d5-71f324fb432f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "temp_failure = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_FAILURE_CASES_PARQ_SEC_APP')\n",
    "# temp_failure_csv = Path(r'/Users/yo_macbook/Documents/app_data/dropbox_13f_files/processed_tables/TR_01_FAILURE_CASES_CSV')\n",
    "final_failure = Path(r'/Users/yo_macbook/Documents/app_data/TR_02_FAILURE_CASES')\n",
    "\n",
    "copy_files(temp_failure, final_failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4857ea9d-9e73-4029-861f-ceed6e9e8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "copy_files = lambda folder_from, folder_to: [shutil.copy(file, folder_to) for file in Path(folder_from).iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab15c8-6422-4103-8146-680c1902a546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from humanize import naturalsize\n",
    "\n",
    "# size = final_parq.stat().st_size\n",
    "\n",
    "# print(naturalsize(size))\n",
    "final_parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f196ba4-c465-400c-9034-2d255c7a7cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = []\n",
    "folder_size = lambda folder: size.extend(file.stat().st_size for file in folder.iterdir())\n",
    "folder_size(final_parq)\n",
    "print(naturalsize(sum(size)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6447ed-0f70-4824-b7b4-93f8a9ea8956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554403f5-d09e-49d8-b1f9-9fb2db4dee38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "np.random.seed(123)\n",
    "\n",
    "fake = Faker()\n",
    "n = 10\n",
    "\n",
    "# categories = [fake.word() for _ in range(3)]\n",
    "\n",
    "# # Create a list of categories with repetition\n",
    "# categories = np.repeat(categories, [3, 4, 3])\n",
    "\n",
    "# Shuffle the categories randomly\n",
    "np.random.shuffle(categories)\n",
    "\n",
    "data = {\n",
    "    'price': np.random.rand(n),\n",
    "    'amount': np.random.randint(0, 10, n),\n",
    "    'rank': range(n),\n",
    "    'date': pd.date_range(start='2022-01-01', periods=n),\n",
    "    'city': [fake.city() for _ in range(n)],\n",
    "    'name': [fake.name() for _ in range(n)], \n",
    "    'category': np.repeat([fake.country() for _ in range(3)], [3, 4, 3])\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(data)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3c06e-4983-4042-8d6d-af881f213f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_list1 = [fake.name() for _ in range(10)]\n",
    "rand_list2 = [fake.name() for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdccd3a-ac0c-493d-9963-e3c8d945e8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02ed01-bbfd-4d28-a3de-0e18e3b08e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(set(rand_list1) & set(rand_list2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87df31c-4c03-4c7b-ada7-aed3c003576c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rand = np.random.RandomState(42)\n",
    "df = pd.DataFrame({\"xval\": range(100), \"yval\": rand.randn(100).cumsum()})\n",
    "\n",
    "slider = alt.binding_range(min=0, max=100, step=1, name=\"cutoff:\")\n",
    "selector = alt.selection_single(\n",
    "    name=\"SelectorName\", fields=[\"cutoff\"], bind=slider, init={\"cutoff\": 50}\n",
    ")\n",
    "\n",
    "alt.Chart(df).mark_point().encode(\n",
    "    x=\"xval\",\n",
    "    y=\"yval\",\n",
    "    color=alt.condition(\n",
    "        alt.datum.xval < selector.cutoff, alt.value(\"red\"), alt.value(\"blue\")\n",
    "    ),\n",
    ").add_selection(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51271c-3fb0-48df-a6f9-68f3b4507156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "source = data.stocks()\n",
    "source.symbol.value_counts()\n",
    "source[\"category\"] = \"software\"\n",
    "source.loc[source[\"symbol\"].isin([\"AAPL\", \"IBM\"]), \"category\"] = \"hardware\"\n",
    "\n",
    "cat_values = list(source[\"category\"].unique())\n",
    "sym_values = list(source[\"symbol\"].unique())\n",
    "\n",
    "cat_options = [None] + cat_values\n",
    "sym_options = [None] + sym_values\n",
    "\n",
    "cat_labels = [\"All\"] + cat_values\n",
    "sym_labels = [\"All\"] + sym_values\n",
    "\n",
    "dropdown_category = alt.binding_select(options=cat_options, labels=cat_labels, name=\" \")\n",
    "dropdown_symbol = alt.binding_select(options=sym_options, labels=sym_labels, name=\" \")\n",
    "\n",
    "selection_category = alt.selection_single(\n",
    "    fields=[\"category\"], bind=dropdown_category, name=\"cat\"\n",
    ")\n",
    "selection_symbol = alt.selection_single(\n",
    "    fields=[\"symbol\"], bind=dropdown_symbol, name=\"sym\"\n",
    ")\n",
    "\n",
    "op_condition = {\n",
    "    \"condition\": {\n",
    "        \"test\": {\"and\": [{\"selection\": \"cat\"}, {\"selection\": \"sym\"}]},\n",
    "        \"value\": 1,\n",
    "    },\n",
    "    \"value\": 0.1,\n",
    "}\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(source)\n",
    "    .mark_line()\n",
    "    .encode(x=\"date\", y=\"price\", color=\"symbol\", opacity=op_condition)\n",
    "    .add_selection(selection_symbol, selection_category)\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f6ef3-699b-4314-8ba8-24a225b123ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "final_parq_both = Path(r\"/Users/yo_macbook/Documents/app_data/TR_02_FINAL_PARQ_BOTH_1999_2023_02_17\")\n",
    "\n",
    "cik_list_parq = sorted(set([file.stem.split('-')[0] for file in final_parq_both.iterdir() if file.suffix == '.parquet']))\n",
    "cik_list_parq = sorted(set([file.stem.split('-')[0] for file in final_parq_both.iterdir() if file.suffix == '.txt']))\n",
    "len(cik_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eefc85-97f3-47e9-82a2-52c2cd70f290",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "[print(el) for index, el in enumerate(set(cik_list)) if index<=n-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff655c-2709-432b-8811-46555931f77b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## List **new** filings to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9303ea-c4fd-4230-bdac-d1d2e52b4eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "\n",
    "extract_txt_stem = lambda txt_folder: [file.stem for file in txt_folder.glob('*/*.txt')]\n",
    "extract_parq_stem = lambda parq_folder: ['-'.join(itemgetter(1, 2, 3)(file.stem.split('-'))) for file in parq_folder.iterdir() if file.suffix=='.parquet']\n",
    "\n",
    "txt_filings_dir = Path(r\"/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings\")\n",
    "parquet_filings_dir = Path(r'/Users/yo_macbook/Documents/app_data/TR_02_FINAL_PARQ_BOTH_1999_2023_02_17')\n",
    "\n",
    "txt_file_stems = extract_txt_stem(txt_filings_dir)\n",
    "parquet_file_stems = extract_parq_stem(parquet_filings_dir)\n",
    "\n",
    "files_diff = list(set(txt_file_stems) - set(parquet_file_stems))\n",
    "# filtering out files with `fdate` from years 98, 99 or in general earlier than 2023\n",
    "files_diff = [file for file in files_diff if int(itemgetter(1)(file.split('-'))) >=23 and int(itemgetter(1)(file.split('-'))) not in [98, 99]]\n",
    "\n",
    "extract_txt_paths = lambda txt_folder: [file for file in txt_folder.glob('*/*.txt')]\n",
    "txt_paths_all = extract_txt_paths(txt_filings_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab8c1b-c820-4bd3-996e-da375fbdf07e",
   "metadata": {},
   "source": [
    "## **New version** of the big mama of the parsing function from `txt` filings to `parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edb2260e-5981-4893-9980-90a64b3a6461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_txt_filings(txt_folder, parq_folder, dir_failures, data_load_dir, cutoff_date = '2023-02-17'):\n",
    "    extract_txt_stem = lambda txt_folder: [file.stem for file in txt_folder.glob('*/*.txt')]\n",
    "    extract_parq_stem = lambda parq_folder: ['-'.join(itemgetter(1, 2, 3)(file.stem.split('-'))) for file in parq_folder.iterdir() if file.suffix=='.parquet']\n",
    "\n",
    "    txt_file_stems = extract_txt_stem(txt_folder)\n",
    "    parquet_file_stems = extract_parq_stem(parq_folder)\n",
    "\n",
    "    files_diff = list(set(txt_file_stems) - set(parquet_file_stems))\n",
    "    # filtering out files with `fdate` from years 98, 99 or in general earlier than 2023\n",
    "    files_diff = [file for file in files_diff if int(itemgetter(1)(file.split('-'))) >=23 and int(itemgetter(1)(file.split('-'))) not in [98, 99]]\n",
    "\n",
    "    extract_txt_paths = lambda txt_folder: [file for file in txt_folder.glob('*/*.txt') if file.stem in files_diff]\n",
    "    txt_paths_diff = extract_txt_paths(txt_folder)\n",
    "    print(f'files to parse: {len(txt_paths_diff)}')\n",
    "    loaded_ciks = []\n",
    "    data_load_run = datetime.utcnow().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    for file in txt_paths_diff:\n",
    "        data_load_data = end_to_end_parsing(file_path=file, directory_parquet=parq_folder, failures_parq_dir=dir_failures, data_load_run=data_load_run, cutoff_date=cutoff_date)\n",
    "        loaded_ciks.append(data_load_data)\n",
    "    loaded_ciks = list(filter(None, loaded_ciks))\n",
    "    if len(loaded_ciks) > 0: \n",
    "        pd.DataFrame(loaded_ciks).to_parquet(data_load_dir.joinpath(data_load_run+'.parquet'))\n",
    "        \n",
    "    parse_txt_filings.n_filings_saved = len(loaded_ciks)\n",
    "    print(f'Total filings written to parquet: {parse_txt_filings.n_filings_saved}')\n",
    "    \n",
    "    parse_txt_filings.ciks_saved = list(set(list(map(itemgetter('cik'), loaded_ciks))))\n",
    "    print(f'cik written to parquet: {parse_txt_filings.ciks_saved}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "532f34fc-3a78-454d-808d-f22e5e73835a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files to parse: 96\n",
      "Total filings written to parquet: 0\n",
      "Total filings written to parquet: []\n",
      "CPU times: user 3.03 s, sys: 1.34 s, total: 4.36 s\n",
      "Wall time: 7.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "txt_filings_dir = Path(r\"/Users/yo_macbook/Documents/app_data/TR_01_FILINGS_TXT/filings\")\n",
    "parquet_filings_dir = Path(r'/Users/yo_macbook/Documents/app_data/TR_02_1_FILINGS_PARQ')\n",
    "failure_cases = Path(r'/Users/yo_macbook/Documents/app_data/TR_02_2_PANDERA_FAILED_ENTRIES')\n",
    "data_load_dir = Path(r'/Users/yo_macbook/Documents/app_data/TR_02_3_DATA_LOAD_LOGS')\n",
    "\n",
    "test_parquet = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/test_parq')\n",
    "\n",
    "parse_txt_filings(txt_filings_dir, parquet_filings_dir, failure_cases, data_load_dir, cutoff_date = '2023-02-17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8a1bf19-4f96-43d3-be8a-17b12af6735f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[947484, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse_txt_filings.n_filings_saved\n",
    "l = [{'data_load_run': '2023-03-22-15-03',\n",
    "  'cik': 947484,\n",
    "  'accession_number': '0000947484-23-000016',\n",
    "  'qa': 'no'},\n",
    "    {'data_load_run': '2023-03-22-15-03',\n",
    "  'cik': 000,\n",
    "  'accession_number': '0000947484-23-000016',\n",
    "  'qa': 'no'}]\n",
    "# print([getattr(d,'cik') for d in l])\n",
    "\n",
    "# d = {'data_load_run': '2023-03-22-15-03',\n",
    "#   'cik': 947484,\n",
    "#   'accession_number': '0000947484-23-000016',\n",
    "#   'qa': 'no'}\n",
    "list(map(itemgetter('cik'), l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59485f07-7042-43b8-8084-04e4f23e572b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 745 ms, sys: 2 s, total: 2.74 s\n",
      "Wall time: 931 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandera as pa\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "\n",
    "## Function to end-to-end parse the raw `txt` filings, write to a pandas dataframe\n",
    "## and save to a `parquet` file. Only `xml` based filings are dealt with. \n",
    "## The filings prior `xml` format are ignored and instead we use the `csv` filings from dropbox\n",
    "\n",
    "## no division by 1000\n",
    "## cusip transformed to upper case\n",
    "## uses `pandera` to remove `cusip` != length of 9 (as non compliant)\n",
    "## uses `pandera` to remove `values` or `shares` != 0 (as non compliant)\n",
    "## the code writes bad data into a separate dataframes per filings\n",
    "## transformation where we group multiple `cusip` into one only after we have removed \n",
    "# the ones with zeros in `value` or `shares` \n",
    "\n",
    "\n",
    "def end_to_end_parsing(file_path, directory_parquet, failures_parq_dir, data_load_run, cutoff_date = '2023-02-17'):\n",
    "    \"\"\"\n",
    "    func that takes a txt filing's path and parses it to a parquet file\n",
    "    in the `directory_parquet`. It uses `pandera` lazy mode to filter out\n",
    "    bad cusip or empty/nan/zero shares or values. Bad records are written\n",
    "    to the `failures_parq_dir` for a later analysis or correction.\n",
    "    \n",
    "    \"\"\"\n",
    "    def work_content_extract(file_path):\n",
    "        \"\"\"extracting 1. xml or not flag, 2. xml part of filings, SEC header part\"\"\"\n",
    "        open_file = open(file_path, \"r\")\n",
    "        file_content = open_file.read()\n",
    "        open_file.close()\n",
    "\n",
    "        # find block of text between two words/tags/strings\n",
    "        start_header = \"<SEC-HEADER>\"\n",
    "        end_header = \"</SEC-HEADER>\"\n",
    "        block_header_search = re.compile(f\"{start_header}.*?{end_header}\", re.DOTALL)\n",
    "        pages_header = re.findall(block_header_search, file_content)\n",
    "\n",
    "        # check if file is XML or not\n",
    "        xml_tag_search = re.compile(r\"<xml>\", flags=re.IGNORECASE)\n",
    "        if xml_tag_search.search(file_content):\n",
    "            xml_flag = \"YES\"\n",
    "\n",
    "            # xml block search\n",
    "            start_xml = \"<XML\\>\"\n",
    "            end_xml = \"<\\/XML\\>\"\n",
    "            block_xml_search = re.compile(f\"{start_xml}(.*?){end_xml}\", re.DOTALL)\n",
    "            pages_xml = re.findall(block_xml_search, file_content)\n",
    "\n",
    "            if len(pages_xml) == 0:\n",
    "                reportType = \"\"\n",
    "                document0 = \"\"\n",
    "                document1 = \"\"\n",
    "            elif len(pages_xml) == 1:\n",
    "                document0 = bs(pages_xml[0], \"xml\")\n",
    "                coverPage = document0.find(\"coverPage\")\n",
    "                reportType = coverPage.find(\"reportType\").text.strip()\n",
    "                document1 = \"\"\n",
    "            if len(pages_xml) > 1:\n",
    "                document0 = bs(pages_xml[0], \"xml\")\n",
    "                coverPage = document0.find(\"coverPage\")\n",
    "                reportType = coverPage.find(\"reportType\").text.strip()\n",
    "                document1 = bs(pages_xml[1], \"xml\")\n",
    "\n",
    "        else:\n",
    "            xml_flag = \"NO\"\n",
    "            pages_xml = []\n",
    "            reportType = \"\"\n",
    "            document1 = \"\"\n",
    "\n",
    "        return pages_header, pages_xml, xml_flag, reportType, document1\n",
    "\n",
    "    def parse_institutionalInvestorInfo(file_path):\n",
    "        dataDictionary = dict()\n",
    "        dataDictionary[\"edgar_path\"] = file_path\n",
    "        periodOfReport = datetime.strptime(\n",
    "            re.compile(r\"(?<=CONFORMED PERIOD OF REPORT:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip(),\n",
    "            \"%Y%m%d\",\n",
    "        ).date()\n",
    "        report_Year, report_Quarter = (\n",
    "            periodOfReport.year,\n",
    "            (periodOfReport.month - 1) // 3 + 1,\n",
    "        )\n",
    "\n",
    "        dataDictionary[\"accessionNumber\"] = (\n",
    "            re.compile(r\"(?<=ACCESSION NUMBER:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"cikManager\"] = (\n",
    "            re.compile(r\"(?<=CENTRAL INDEX KEY:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"managerName\"] = (\n",
    "            re.compile(r\"(?<=COMPANY CONFORMED NAME:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"periodOfReport\"] = periodOfReport\n",
    "        dataDictionary[\"report_Quarter\"] = report_Quarter\n",
    "        dataDictionary[\"report_Year\"] = report_Year\n",
    "        dataDictionary[\"submissionType\"] = (\n",
    "            re.compile(r\"(?<=CONFORMED SUBMISSION TYPE:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"filedAsOfDate\"] = datetime.strptime(\n",
    "            re.compile(r\"(?<=FILED AS OF DATE:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip(),\n",
    "            \"%Y%m%d\",\n",
    "        ).date()\n",
    "        dataDictionary[\"xml_flag\"] = xml_flag\n",
    "        dataDictionary[\"created_at\"] = datetime.now()\n",
    "        dataDictionary[\"updated_at\"] = datetime.now()\n",
    "\n",
    "        # dataDictionary = dict()\n",
    "        dataDictionary[\"edgar_path\"] = file_path\n",
    "        if xml_flag == \"NO\":\n",
    "\n",
    "            dataDictionary[\"isAmendment\"] = \"\"\n",
    "            dataDictionary[\"amendmentType\"] = \"\"\n",
    "            dataDictionary[\"entryTotal\"] = int(\"0\" + \"\")\n",
    "            dataDictionary[\"valueTotal\"] = float(\"0.0\" + \"\")\n",
    "            dt = pd.DataFrame.from_dict([dataDictionary])\n",
    "\n",
    "        else:\n",
    "            if len(pages_xml) == 0:\n",
    "                dataDictionary[\"isAmendment\"] = \"\"\n",
    "                dataDictionary[\"amendmentType\"] = \"\"\n",
    "                dataDictionary[\"entryTotal\"] = int(\"0\" + \"\")\n",
    "                dataDictionary[\"valueTotal\"] = float(\"0.0\" + \"\")\n",
    "                dt = pd.DataFrame.from_dict([dataDictionary])\n",
    "\n",
    "            else:\n",
    "                document = bs(pages_xml[0], \"xml\")\n",
    "                # get sections\n",
    "                coverPage = document.find(\"coverPage\")\n",
    "                summaryPage = document.find(\"summaryPage\")\n",
    "                # get data\n",
    "                if coverPage.find(\"isAmendment\") is not None:\n",
    "\n",
    "                    if coverPage.find(\"isAmendment\").text.strip() == \"true\":\n",
    "                        dataDictionary[\"isAmendment\"] = coverPage.find(\n",
    "                            \"isAmendment\"\n",
    "                        ).text.strip()\n",
    "                        if coverPage.find(\"amendmentType\"):\n",
    "                            dataDictionary[\"amendmentType\"] = coverPage.find(\n",
    "                                \"amendmentType\"\n",
    "                            ).text.strip()\n",
    "                        else:\n",
    "                            dataDictionary[\"amendmentType\"] = \"\"\n",
    "                    else:\n",
    "                        dataDictionary[\"isAmendment\"] = coverPage.find(\n",
    "                            \"isAmendment\"\n",
    "                        ).text.strip()\n",
    "                        dataDictionary[\"amendmentType\"] = \"\"\n",
    "                else:\n",
    "                    dataDictionary[\"isAmendment\"] = \"\"\n",
    "                    dataDictionary[\"amendmentType\"] = \"\"\n",
    "\n",
    "                if summaryPage is not None:\n",
    "                    if summaryPage.find(\"tableEntryTotal\").text.strip():\n",
    "                        dataDictionary[\"entryTotal\"] = int(\n",
    "                            float(summaryPage.find(\"tableEntryTotal\").text.strip() + \"0.0\")\n",
    "                        )\n",
    "                    else:\n",
    "                        dataDictionary[\"entryTotal\"] = int(\"0\" + \"\")\n",
    "\n",
    "\n",
    "                    if summaryPage.find(\"tableValueTotal\").text.strip():\n",
    "                        dataDictionary[\"valueTotal\"] = float(\n",
    "                        summaryPage.find(\"tableValueTotal\").text.strip()\n",
    "                    + \"0.0\")\n",
    "\n",
    "                    else: dataDictionary[\"valueTotal\"] = float(\"0.0\" + \"\")\n",
    "                else:\n",
    "                    dataDictionary[\"entryTotal\"], dataDictionary[\"valueTotal\"] = int(\n",
    "                        \"0\" + \"\"\n",
    "                    ), float(\"0.0\" + \"\")\n",
    "\n",
    "                # create dataframe\n",
    "                dt = pd.DataFrame.from_dict([dataDictionary])\n",
    "\n",
    "        return dt\n",
    "\n",
    "    def parse_institutionalInvestorPortfolio(file_path):\n",
    "        check = re.compile(\"13F HOLDINGS REPORT|13F COMBINATION REPORT\")\n",
    "        if check.search(reportType) is not None and len(pages_xml) > 1:\n",
    "\n",
    "            portfolio = list()\n",
    "            # find all securities held\n",
    "\n",
    "            securities = document1.find_all(\"infoTable\")\n",
    "            for row in securities:\n",
    "                portfolioDict = dict()\n",
    "                portfolioDict[\"edgar_path\"] = file_path\n",
    "                portfolioDict[\"cusip\"] = row.find(\"cusip\").text.strip()\n",
    "                portfolioDict[\"nameOfIssuer\"] = row.find(\"nameOfIssuer\").text.strip()\n",
    "                portfolioDict[\"titleOfClass\"] = row.find(\"titleOfClass\").text.strip()\n",
    "                portfolioDict[\"sharesValue\"] = float(\n",
    "                    row.find(\"value\").text.strip()\n",
    "                )\n",
    "                portfolioDict[\"sharesHeldAtEndOfQtr\"] = int(\n",
    "                    float(row.find(\"sshPrnamt\").text.strip())\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    portfolioDict[\"sharePriceAtEndOfQtr\"] = round(\n",
    "                        portfolioDict[\"sharesValue\"]\n",
    "                        / portfolioDict[\"sharesHeldAtEndOfQtr\"],\n",
    "                        2,\n",
    "                    )\n",
    "                except ZeroDivisionError:\n",
    "                    portfolioDict[\"sharePriceAtEndOfQtr\"] = float(int(\"0\" + \"\"))\n",
    "\n",
    "                portfolioDict[\"shares_bonds\"] = row.find(\"sshPrnamtType\").text.strip()\n",
    "\n",
    "                if row.find(\"putCall\") is not None:\n",
    "                    portfolioDict[\"putCall\"] = row.find(\"putCall\").text.strip()\n",
    "                else:\n",
    "                    portfolioDict[\"putCall\"] = \"\"\n",
    "\n",
    "                portfolio.append(pd.DataFrame.from_dict([portfolioDict]))\n",
    "\n",
    "            # concatanate secutires\n",
    "            dtPortfolio = pd.concat(portfolio, sort=False, ignore_index=True)\n",
    "        else:\n",
    "            dtPortfolio = pd.DataFrame.from_dict(\n",
    "                [\n",
    "                    {\n",
    "                        \"cusip\": \"\",\n",
    "                        \"nameOfIssuer\": \"\",\n",
    "                        \"titleOfClass\": \"\",\n",
    "                        \"sharesValue\": float(\"0.0\" + \"\"),\n",
    "                        \"sharesHeldAtEndOfQtr\": int(\"0\" + \"\"),\n",
    "                        \"sharePriceAtEndOfQtr\": float(\"0.0\" + \"\"),\n",
    "                        \"shares_bonds\": \"\",\n",
    "                        \"putCall\": \"\",\n",
    "                        \"edgar_path\": file_path,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return dtPortfolio\n",
    "\n",
    "    pages_header, pages_xml, xml_flag, reportType, document1 = work_content_extract(\n",
    "        file_path\n",
    "    )\n",
    "    df1, df2 = parse_institutionalInvestorInfo(\n",
    "        file_path\n",
    "    ), parse_institutionalInvestorPortfolio(file_path)\n",
    "\n",
    "    data = pd.merge(df1, df2, on=\"edgar_path\")\n",
    "    data = data.assign(dsource='sec_app')    \n",
    "    data = data.rename(columns={'accessionNumber': 'accession_number', \"cikManager\": 'cik', \"managerName\":'cik_name','xml_flag':'type',\n",
    "           \"periodOfReport\": 'rdate', \"submissionType\": 'submission_type', \"filedAsOfDate\":'fdate', \"entryTotal\": \"entry_total\",\n",
    "           \"valueTotal\": \"value_total\", 'putCall':'put_call', \"sharesValue\":'value', \"sharesHeldAtEndOfQtr\":'shares',\n",
    "           \"securityType\": \"security_type\", 'titleOfClass':'title_of_class', 'nameOfIssuer':'name_of_issuer' , \"edgar_path\":'file'})\n",
    "\n",
    "    column_names = [\n",
    "        \"accession_number\",\n",
    "        \"cik\",\n",
    "        \"cik_name\",\n",
    "        \"rdate\",\n",
    "        \"submission_type\",\n",
    "        \"fdate\",\n",
    "        \"entry_total\",\n",
    "        \"value_total\",\n",
    "        \"cusip\",\n",
    "        \"name_of_issuer\",\n",
    "        \"title_of_class\",\n",
    "        \"value\",\n",
    "        \"shares\",\n",
    "        \"shares_bonds\",\n",
    "        \"put_call\",\n",
    "        \"type\",\n",
    "        \"file\",\n",
    "        'dsource',\n",
    "        \"created_at\",\n",
    "        'updated_at',\n",
    "        'data_load_run'\n",
    "    ]\n",
    "    data = data.astype(\n",
    "        {\n",
    "            \"accession_number\": str,\n",
    "            \"cik\": \"Int64\",\n",
    "            'cik_name': str,\n",
    "            \"rdate\": \"datetime64[ns]\",\n",
    "            \"submission_type\": str,\n",
    "            \"fdate\": \"datetime64[ns]\",\n",
    "            \"entry_total\": \"Int64\",\n",
    "            \"value_total\": \"float64\",\n",
    "            \"cusip\": str,\n",
    "            \"name_of_issuer\": str,\n",
    "            \"title_of_class\": str,\n",
    "            \"value\": \"float64\",\n",
    "            \"shares\": \"float64\",\n",
    "            \"shares_bonds\": str,\n",
    "            \"put_call\": str,\n",
    "            \"type\": str,\n",
    "            \"file\": str,\n",
    "            'dsource': str,\n",
    "            \"created_at\": \"datetime64[ns]\",\n",
    "            \"updated_at\": \"datetime64[ns]\"\n",
    "        }\n",
    "    )\n",
    "    attributes = {\n",
    "        \"cik\": \"first\",\n",
    "        \"cik_name\": \"first\",\n",
    "        \"rdate\": \"first\",       \n",
    "        \"fdate\": \"first\",\n",
    "        \"cusip\": \"first\",\n",
    "        \"name_of_issuer\": \"first\",        \n",
    "        \"value\": \"sum\",\n",
    "        \"shares\": \"sum\",\n",
    "        \"title_of_class\": \"first\",\n",
    "        \"shares_bonds\": \"first\",\n",
    "        \"put_call\": \"first\",\n",
    "        \"accession_number\": \"first\",\n",
    "        \"submission_type\": \"first\",\n",
    "        \"type\": \"first\",\n",
    "        \"file\": \"first\",\n",
    "        'dsource': 'first',\n",
    "        \"entry_total\": \"first\",\n",
    "        \"value_total\": \"first\",\n",
    "        \"created_at\": \"first\",\n",
    "        \"updated_at\": \"first\",\n",
    "        'data_load_run': 'first'\n",
    "    }\n",
    "    data = data.assign(cusip=data.cusip.str.upper(),\n",
    "                       data_load_run=data_load_run)\n",
    "    data = data.reindex(columns=column_names)\n",
    "   \n",
    "    ## pandera code\n",
    "    validation_schema = pa.DataFrameSchema({\n",
    "    \"cusip\": pa.Column(str,\n",
    "                       pa.Check(lambda s: s.str.len() == 9),\n",
    "                       required=True, nullable=False),\n",
    "    \"value\":  pa.Column(float, pa.Check(lambda s: s != 0.0), required=True, nullable=False),\n",
    "    \"shares\": pa.Column(float, pa.Check(lambda s: s != 0.0), required=True, nullable=False)\n",
    "        })\n",
    "    if data.head(1).type.squeeze() != 'NO' and data.head(1).fdate.squeeze() > pd.to_datetime(cutoff_date,format='%Y-%m-%d'):\n",
    "        try:\n",
    "            validation_schema.validate(data, lazy=True)\n",
    "            if not data.empty:\n",
    "                # group cusips\n",
    "                df2 = data.groupby([\"cusip\"], as_index=False).agg(attributes)\n",
    "                df2.to_parquet(\n",
    "                    os.path.join(\n",
    "                        directory_parquet,\n",
    "                        f\"{df2.head(1).cik[0]}-{df2.head(1).accession_number[0]}-{df2.head(1).fdate[0].strftime('%Y-%m-%d')}.parquet\",\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                # return df2\n",
    "            else: pd.DataFrame()\n",
    "\n",
    "        except pa.errors.SchemaErrors as e:\n",
    "            failure_cases = e.failure_cases\n",
    "            failure_cases = (failure_cases.assign(df_file=data.file,\n",
    "                                                 df_cik=data.cik,\n",
    "                                                 df_accession_number=data.accession_number,\n",
    "                                                 df_rdate=data.rdate,\n",
    "                                                 df_fdate=data.fdate,\n",
    "                                                 df_value=data.value)).astype({'failure_case':str})\n",
    "            # print(failure_cases)\n",
    "            failure_cases.to_parquet(Path.joinpath(failures_parq_dir, \\\n",
    "                                                   f\"bad-{data.head(1).cik[0]}-{data.head(1).accession_number[0]}-{data.head(1).fdate[0].strftime('%Y-%m-%d')}.parquet\"))\n",
    "            data = data[~data.index.isin(failure_cases[\"index\"])]\n",
    "            \n",
    "            if not data.empty:\n",
    "                # group cusips\n",
    "                df2 = data.groupby([\"cusip\"], as_index=False).agg(attributes)\n",
    "                df2.to_parquet(\n",
    "                    os.path.join(\n",
    "                        directory_parquet,\n",
    "                        f\"{df2.head(1).cik[0]}-{df2.head(1).accession_number[0]}-{df2.head(1).fdate[0].strftime('%Y-%m-%d')}.parquet\"))\n",
    "                \n",
    "\n",
    "            else: pd.DataFrame()\n",
    "        if 'cik' and 'accession_number' in df2.columns: return {'data_load_run':data_load_run,'cik':df2.head(1).cik[0], 'accession_number': df2.head(1).accession_number[0], 'qa':'no'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb869d-861a-498c-9400-de9b4c473e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = ['a', 'b']\n",
    "l = ['a', 'b', 'c', 'z']\n",
    "if 'b' or 'a' in l: print('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da298687-e2ed-45bd-828b-645f25f7e8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# file_path = Path(r'/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings/1009027/0001009027-23-000001.txt')\n",
    "file_path = Path(r'/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings/1011659/0001085146-23-001425.txt')\n",
    "\n",
    "\n",
    "txt_filings_dir = Path(r\"/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings\")\n",
    "parquet_filings_dir = Path(r'/Users/yo_macbook/Documents/app_data/TR_02_PARQ_ALL')\n",
    "failure_cases = Path(r'/Users/yo_macbook/Documents/app_data/TR_02_FAILURE_CASES')\n",
    "data_load_run = datetime.utcnow().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "test_parquet = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/test_parq')\n",
    "\n",
    "end_to_end_parsing(file_path, test_parquet, failure_cases, data_load_run, cutoff_date = '2023-02-17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e278e8-256f-45b4-b00e-cba148b63dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = end_to_end_parsing(file_path, test_parquet, failure_cases, data_load_run, cutoff_date = '2023-02-17')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817aea0c-67a9-432f-b221-917b8883f365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find duplicates in a list\n",
    "freq = {}\n",
    "\n",
    "for element in parquet_files:\n",
    "    if element in freq:\n",
    "        freq[element] += 1\n",
    "    else:\n",
    "        freq[element] = 1\n",
    "\n",
    "duplicates = [key for key,value in freq.items() if value > 1 and int(itemgetter(1)(key.split('-'))) >=23 and int(itemgetter(1)(key.split('-'))) not in [98, 99]]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b9caf-f912-4652-a556-5ca98fe3d702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\"},\n",
    "    {\"id\": 2, \"name\": None},\n",
    "    {},\n",
    "    {\"id\": 3, \"name\": \"Bob\"},\n",
    "    None,\n",
    "]\n",
    "\n",
    "def list_comprehension():\n",
    "    return [d for d in data if bool(d)]\n",
    "\n",
    "def filter_function():\n",
    "    return list(filter(None, data))\n",
    "\n",
    "print(\"List comprehension:\", timeit.timeit(list_comprehension, number=1000000))\n",
    "print(\"Filter function:\", timeit.timeit(filter_function, number=1000000))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
