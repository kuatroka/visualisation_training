{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c6bc6f-c794-4870-bdfe-f4ef6fbb2929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 35)\n",
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081d663-130b-40b5-a458-d9de0c75126a",
   "metadata": {},
   "source": [
    "### Parsing `txt` to `parquet` for selected 676 cik and does some cleaning with `pandera`\n",
    "* Casts `cusip` to upper case and cleans empty/zero value or shares\n",
    "* filter outs `cusip` that are non-8 char "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4845c7-4bfd-44da-8145-9e7814a81eb6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# for selected 676 cik, parse txt filings to parquet\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from utils_data_cleaning import end_to_end_parsing\n",
    "\n",
    "selected_cik = [2230,3520,5272,7195,7789,9015,10742,14661,16972,18349,18748,19475,19617,21175,22657,24386,35442,35527,36066,36104,36644,36966,38777,39263,40417,40545,44365,45319,49205,50863,51762,51812,51964,52234,53417,59558,59951,60086,61227,67698,70858,71210,71259,72971,73124,80255,84616,89014,92230,93751,98758,102212,102909,105495,108572,200217,201772,216851,276101,310051,312348,313028,313807,314949,314957,314984,315014,315032,315038,315054,315066,315080,315157,315297,315498,316011,318989,320335,320376,351051,351173,351262,354204,356264,700529,704051,707179,712537,713676,714142,720672,723204,728083,728100,728618,732905,733020,740272,740913,741073,743127,750641,754811,757657,759944,762152,763212,763848,764068,764106,764112,764529,764532,765443,769317,769954,769963,775368,776867,778963,779519,788714,790354,790502,791191,791490,796848,799003,799004,801051,806097,807249,807985,808722,809339,809443,810265,810384,810386,810672,810716,811360,811454,813917,813933,814133,814375,816788,819535,820027,820123,820124,820289,820478,820743,821197,822581,823621,825293,829407,831001,831571,836372,837592,842782,842941,846222,846633,846788,846797,850401,850529,852743,854157,857508,859872,860486,860561,860580,860585,860643,860644,860645,860662,860748,860828,860857,861176,861177,861462,861787,862469,866361,866842,868491,869178,869179,869353,869367,872080,872163,872259,872573,872732,873630,874791,877134,877338,878228,881432,883511,883677,883782,883790,883803,883961,883965,884300,884314,884414,884423,884541,884546,884548,884566,884589,885062,885415,886982,887402,887777,887818,889232,891287,891478,893738,894205,894300,894309,895213,895421,897070,897378,897599,898358,898382,898399,898413,899211,900169,900529,900973,902219,902367,902464,902584,903064,903944,903947,903949,905567,905591,905608,906304,908195,909151,909661,911274,912938,914933,914976,915287,915325,916542,917579,918893,919079,919185,919192,919458,919489,919497,919530,919538,919859,920440,920441,921531,921669,922127,922439,922898,922940,923093,923116,923469,924166,924171,924181,926688,926833,926834,928047,928196,928566,928568,928633,930441,931097,932024,932974,933429,934639,934999,936698,936753,936936,936941,936944,937394,937522,937589,937615,937760,937886,938076,938206,938487,938582,938592,938759,939219,940445,941560,943719,944234,944804,945625,945631,947822,947996,948518,948669,949012,949509,949615,949623,949853,1000097,1000742,1002152,1002672,1002784,1004244,1005354,1005607,1005817,1006364,1006378,1006407,1006435,1007280,1007399,1007524,1008322,1008877,1008894,1008895,1008929,1008937,1009003,1009005,1009012,1009016,1009022,1009076,1009207,1009209,1009232,1009254,1009258,1009262,1010873,1010911,1011443,1011659,1013234,1013536,1013538,1013701,1014306,1014315,1014736,1014738,1015079,1015083,1015086,1015308,1016150,1016287,1016683,1016972,1017115,1017645,1017918,1018331,1018674,1018825,1019231,1020066,1020317,1020580,1020585,1020617,1020918,1021008,1021117,1021223,1021249,1021258,1021642,1021926,1023279,1024716,1025421,1026200,1026710,1027451,1027796,1027817,1029160,1030618,1030815,1031972,1032814,1033225,1033427,1033475,1033505,1033974,1033984,1034184,1034196,1034524,1034541,1034546,1034549,1034642,1034771,1034886,1035350,1035463,1035912,1036248,1036325,1037389,1037558,1037763,1037792,1038661,1039565,1039807,1040190,1040197,1040198,1040210,1040273,1040592,1040762,1041241,1041885,1042046,1044207,1044797,1044905,1044924,1044929,1044936,1046187,1047339,1048921,1049648,1049650,1050442,1050463,1050470,1051359,1052100,1053013,1053054,1053055,1054074,1054425,1054522,1054554,1054677,1055290,1055544,1055963,1055964,1055966,1056053,1056288,1056466,1056488,1056491,1056515,1056516,1056527,1056549,1056559,1056581,1056593,1056807,1056821,1056825,1056827,1056831,1056859,1056958,1056973,1057395,1057439,1058022,1058470,1058800,1059187,1061186,1061768,1062938,1065349,1065350,1066816,1067324,1067926,1067983,1068829,1070134,1071483,1072843,1074027,1074034,1074266,1074273,1076598,1077148,1077583,1078013,1078246,1078658,1078841,1079112,1079114,1079736,1079738,1079930,1080071,1080107,1080117,1080132,1080166,1080171,1080173,1080197,1080201,1080351,1080374,1080380,1080381,1080382,1080386,1080493,1080523,1080628,1080818,1081019,1081198,1082020,1082215,1082327,1082339,1082461,1082491,1082509,1082621,1082917,1083323,1083340,1084207,1084208,1084683,1085041,1085163,1085227,1085601,1085936,1086477,1086483,1086611,1086619,1086762,1086763,1088859,1088875,1088950,1089707,1089755,1089911,1089991,1090413,1091561,1091860,1091923,1092203,1092290,1092351,1092903,1093276,1093589,1094584,1094749,1095836,1096783,1097218,1097278,1097833,1100710,1101250,1102062,1102578,1102598,1103245,1103738,1103804,1103882,1103887,1104186,1104329,1104366,1105468,1105471,1105497,1105837,1105863,1105909,1106129,1106191,1106500,1106505,1106832,1107261,1107310,1108893,1108965,1108969,1109147,1110806,1113629,1114618,1114739,1114928,1115941,1116247,1125727,1125816,1129770,1133219,1134152,1140334,1140771,1142031,1142062,1158583,1389426,1398739,1469219]\n",
    "TR_00_TXT_SEC_APP = Path('/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings')\n",
    "TR_02_TEST_676_CIK_PARQ_SEC_APP = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_TEST_676_CIK_PARQ_SEC_APP')\n",
    "TR_02_FAILURE_CASES_PARQ_SEC_APP = Path(r\"/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_FAILURE_CASES_PARQ_SEC_APP\")\n",
    "\n",
    "filings_parq = TR_02_TEST_676_CIK_PARQ_SEC_APP.glob(\"*.parquet\")\n",
    "f = TR_00_TXT_SEC_APP\n",
    "l = selected_cik\n",
    "subdirs = list(f.glob(\"*\"))\n",
    "subdirs = [d for d in subdirs if d.is_dir() and int(d.name) in l]\n",
    "\n",
    "for subdir in subdirs: # [-20:-1]\n",
    "    filings_txt = list(subdir.glob(\"*.txt\"))\n",
    "    for file in filings_txt:\n",
    "        file_exists = list(TR_02_TEST_676_CIK_PARQ_SEC_APP.glob(f\"{subdir.stem}-{file.stem}*\"))\n",
    "        # Check if the file exists\n",
    "        if len(file_exists) > 0:\n",
    "            continue\n",
    "        else:\n",
    "            # print(\"File does not exist. Converting to parquet...\")\n",
    "            end_to_end_parsing(file, TR_02_TEST_676_CIK_PARQ_SEC_APP, TR_02_FAILURE_CASES_PARQ_SEC_APP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffac596-8947-4040-85a8-5f095166d138",
   "metadata": {},
   "source": [
    "### Parsing `txt` to `parquet` for **ALL** `cik` and does some cleaning with `pandera`\n",
    "* Casts `cusip` to upper case and cleans empty/zero value or shares\n",
    "* filter outs `cusip` that are non-8 char "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ecf6137-e031-4a88-9769-daf08c417239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16h 40min 20s, sys: 3h 40min 3s, total: 20h 20min 23s\n",
      "Wall time: 21h 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parse txt filings to parquet for ALL cik\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from utils_data_cleaning import end_to_end_parsing\n",
    "\n",
    "TR_00_TXT_SEC_APP = Path('/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings')\n",
    "TR_02_ALL_PARQ_SEC_APP = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_ALL_PARQ_SEC_APP')\n",
    "TR_02_FAILURE_CASES_PARQ_SEC_APP = Path(r\"/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_FAILURE_CASES_PARQ_SEC_APP\")\n",
    "\n",
    "# all_filings_parq = TR_02_ALL_PARQ_SEC_APP.glob(\"*.parquet\")\n",
    "subdirs = list(TR_00_TXT_SEC_APP.glob(\"*\"))\n",
    "\n",
    "for subdir in subdirs: # [-20:-1]\n",
    "    filings_txt = list(subdir.glob(\"*.txt\"))\n",
    "    for file in filings_txt:\n",
    "        file_exists = list(TR_02_ALL_PARQ_SEC_APP.glob(f\"{subdir.stem}-{file.stem}*\"))\n",
    "        # Check if the file exists\n",
    "        if len(file_exists) > 0: continue\n",
    "        else: end_to_end_parsing(file, TR_02_ALL_PARQ_SEC_APP, TR_02_FAILURE_CASES_PARQ_SEC_APP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca628589-c88c-4c7a-a4b9-ffbc43cea805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings/1005817/0001387131-14-001346.txt')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3b50d-cc4d-47d9-b1d7-055f01991f63",
   "metadata": {},
   "source": [
    "### Parse individual `txt` file to `parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adfdee53-8492-4a1e-a69f-48da57940c58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "from utils_data_cleaning import end_to_end_parsing\n",
    "\n",
    "file = Path(r'/Volumes/fanpc/app_data/sec_apps_data/speed_test/filings_13f_full/filings/902528/0001178913-20-002231.txt')\n",
    "test_parq = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/test_parq')\n",
    "test_failure = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/test_failure')\n",
    "\n",
    "df_test = end_to_end_parsing(file, test_parq, test_failure)\n",
    "# .sort_values(by='value', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72d39d-7c26-4fbf-b68c-869aac3ba6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_test.head()\n",
    "# df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8327991-8f17-4327-8b23-b6997c1e1701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandera as pa\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "\n",
    "## Function to end-to-end parse txt filings and write to a pandas dataframe\n",
    "# no division by 1000\n",
    "# cusip transformed to upper case\n",
    "# `pandera` is used to filter out empty/zero value or shares and non-8 char cusip\n",
    "\n",
    "def end_to_end_parsing(file_path, dir_parq, failures_dir_parq):\n",
    "    def work_content_extract(file_path):\n",
    "        \"\"\"extracting 1. xml or not flag, 2. xml part of filings, SEC header part\"\"\"\n",
    "        open_file = open(file_path, \"r\")\n",
    "        file_content = open_file.read()\n",
    "        open_file.close()\n",
    "\n",
    "        # find block of text between two words/tags/strings\n",
    "        start_header = \"<SEC-HEADER>\"\n",
    "        end_header = \"</SEC-HEADER>\"\n",
    "        block_header_search = re.compile(f\"{start_header}.*?{end_header}\", re.DOTALL)\n",
    "        pages_header = re.findall(block_header_search, file_content)\n",
    "\n",
    "        # check if file is XML or not\n",
    "        xml_tag_search = re.compile(r\"<xml>\", flags=re.IGNORECASE)\n",
    "        if xml_tag_search.search(file_content):\n",
    "            xml_flag = \"YES\"\n",
    "\n",
    "            # xml block search\n",
    "            start_xml = \"<XML\\>\"\n",
    "            end_xml = \"<\\/XML\\>\"\n",
    "            block_xml_search = re.compile(f\"{start_xml}(.*?){end_xml}\", re.DOTALL)\n",
    "            pages_xml = re.findall(block_xml_search, file_content)\n",
    "\n",
    "            if len(pages_xml) == 0:\n",
    "                reportType = \"\"\n",
    "                document0 = \"\"\n",
    "                document1 = \"\"\n",
    "            elif len(pages_xml) == 1:\n",
    "                document0 = bs(pages_xml[0], \"xml\")\n",
    "                coverPage = document0.find(\"coverPage\")\n",
    "                reportType = coverPage.find(\"reportType\").text.strip()\n",
    "                document1 = \"\"\n",
    "            if len(pages_xml) > 1:\n",
    "                document0 = bs(pages_xml[0], \"xml\")\n",
    "                coverPage = document0.find(\"coverPage\")\n",
    "                reportType = coverPage.find(\"reportType\").text.strip()\n",
    "                document1 = bs(pages_xml[1], \"xml\")\n",
    "\n",
    "        else:\n",
    "            xml_flag = \"NO\"\n",
    "            pages_xml = []\n",
    "            reportType = \"\"\n",
    "            document1 = \"\"\n",
    "\n",
    "        return pages_header, pages_xml, xml_flag, reportType, document1\n",
    "\n",
    "    def parse_institutionalInvestorInfo(file_path):\n",
    "        dataDictionary = dict()\n",
    "        dataDictionary[\"edgar_path\"] = file_path\n",
    "        periodOfReport = datetime.strptime(\n",
    "            re.compile(r\"(?<=CONFORMED PERIOD OF REPORT:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip(),\n",
    "            \"%Y%m%d\",\n",
    "        ).date()\n",
    "        report_Year, report_Quarter = (\n",
    "            periodOfReport.year,\n",
    "            (periodOfReport.month - 1) // 3 + 1,\n",
    "        )\n",
    "\n",
    "        dataDictionary[\"accessionNumber\"] = (\n",
    "            re.compile(r\"(?<=ACCESSION NUMBER:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"cikManager\"] = (\n",
    "            re.compile(r\"(?<=CENTRAL INDEX KEY:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"managerName\"] = (\n",
    "            re.compile(r\"(?<=COMPANY CONFORMED NAME:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"periodOfReport\"] = periodOfReport\n",
    "        dataDictionary[\"report_Quarter\"] = report_Quarter\n",
    "        dataDictionary[\"report_Year\"] = report_Year\n",
    "        dataDictionary[\"submissionType\"] = (\n",
    "            re.compile(r\"(?<=CONFORMED SUBMISSION TYPE:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip()\n",
    "        )\n",
    "        dataDictionary[\"filedAsOfDate\"] = datetime.strptime(\n",
    "            re.compile(r\"(?<=FILED AS OF DATE:).*\")\n",
    "            .search(pages_header[0])\n",
    "            .group(0)\n",
    "            .strip(),\n",
    "            \"%Y%m%d\",\n",
    "        ).date()\n",
    "        dataDictionary[\"xml_flag\"] = xml_flag\n",
    "        dataDictionary[\"created_at\"] = datetime.now()\n",
    "\n",
    "        # dataDictionary = dict()\n",
    "        dataDictionary[\"edgar_path\"] = file_path\n",
    "        if xml_flag == \"NO\":\n",
    "\n",
    "            dataDictionary[\"isAmendment\"] = \"\"\n",
    "            dataDictionary[\"amendmentType\"] = \"\"\n",
    "            dataDictionary[\"entryTotal\"] = int(\"0\" + \"\")\n",
    "            dataDictionary[\"valueTotal\"] = float(\"0.0\" + \"\")\n",
    "            dt = pd.DataFrame.from_dict([dataDictionary])\n",
    "\n",
    "        else:\n",
    "            if len(pages_xml) == 0:\n",
    "                dataDictionary[\"isAmendment\"] = \"\"\n",
    "                dataDictionary[\"amendmentType\"] = \"\"\n",
    "                dataDictionary[\"entryTotal\"] = int(\"0\" + \"\")\n",
    "                dataDictionary[\"valueTotal\"] = float(\"0.0\" + \"\")\n",
    "                dt = pd.DataFrame.from_dict([dataDictionary])\n",
    "\n",
    "            else:\n",
    "                document = bs(pages_xml[0], \"xml\")\n",
    "                # get sections\n",
    "                coverPage = document.find(\"coverPage\")\n",
    "                summaryPage = document.find(\"summaryPage\")\n",
    "                # get data\n",
    "                if coverPage.find(\"isAmendment\") is not None:\n",
    "\n",
    "                    if coverPage.find(\"isAmendment\").text.strip() == \"true\":\n",
    "                        dataDictionary[\"isAmendment\"] = coverPage.find(\n",
    "                            \"isAmendment\"\n",
    "                        ).text.strip()\n",
    "                        if coverPage.find(\"amendmentType\"):\n",
    "                            dataDictionary[\"amendmentType\"] = coverPage.find(\n",
    "                                \"amendmentType\"\n",
    "                            ).text.strip()\n",
    "                        else:\n",
    "                            dataDictionary[\"amendmentType\"] = \"\"\n",
    "                    else:\n",
    "                        dataDictionary[\"isAmendment\"] = coverPage.find(\n",
    "                            \"isAmendment\"\n",
    "                        ).text.strip()\n",
    "                        dataDictionary[\"amendmentType\"] = \"\"\n",
    "                else:\n",
    "                    dataDictionary[\"isAmendment\"] = \"\"\n",
    "                    dataDictionary[\"amendmentType\"] = \"\"\n",
    "\n",
    "                if summaryPage is not None:\n",
    "                    if summaryPage.find(\"tableEntryTotal\").text.strip():\n",
    "                        dataDictionary[\"entryTotal\"] = int(\n",
    "                            float(summaryPage.find(\"tableEntryTotal\").text.strip() + \"0.0\")\n",
    "                        )\n",
    "                    else:\n",
    "                        dataDictionary[\"entryTotal\"] = int(\"0\" + \"\")\n",
    "\n",
    "\n",
    "                    if summaryPage.find(\"tableValueTotal\").text.strip():\n",
    "                        dataDictionary[\"valueTotal\"] = float(\n",
    "                        summaryPage.find(\"tableValueTotal\").text.strip()\n",
    "                    + \"0.0\")\n",
    "\n",
    "                    else: dataDictionary[\"valueTotal\"] = float(\"0.0\" + \"\")\n",
    "                else:\n",
    "                    dataDictionary[\"entryTotal\"], dataDictionary[\"valueTotal\"] = int(\n",
    "                        \"0\" + \"\"\n",
    "                    ), float(\"0.0\" + \"\")\n",
    "\n",
    "                # create dataframe\n",
    "                dt = pd.DataFrame.from_dict([dataDictionary])\n",
    "\n",
    "        return dt\n",
    "\n",
    "    def parse_institutionalInvestorPortfolio(file_path):\n",
    "        check = re.compile(\"13F HOLDINGS REPORT|13F COMBINATION REPORT\")\n",
    "        if check.search(reportType) is not None and len(pages_xml) > 1:\n",
    "\n",
    "            portfolio = list()\n",
    "            # find all securities held\n",
    "\n",
    "            securities = document1.find_all(\"infoTable\")\n",
    "            for row in securities:\n",
    "                portfolioDict = dict()\n",
    "                portfolioDict[\"edgar_path\"] = file_path\n",
    "                portfolioDict[\"cusip\"] = row.find(\"cusip\").text.strip()\n",
    "                portfolioDict[\"nameOfIssuer\"] = row.find(\"nameOfIssuer\").text.strip()\n",
    "                portfolioDict[\"titleOfClass\"] = row.find(\"titleOfClass\").text.strip()\n",
    "                portfolioDict[\"sharesValue\"] = float(\n",
    "                    row.find(\"value\").text.strip()\n",
    "                )\n",
    "                portfolioDict[\"sharesHeldAtEndOfQtr\"] = int(\n",
    "                    float(row.find(\"sshPrnamt\").text.strip())\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    portfolioDict[\"sharePriceAtEndOfQtr\"] = round(\n",
    "                        portfolioDict[\"sharesValue\"]\n",
    "                        / portfolioDict[\"sharesHeldAtEndOfQtr\"],\n",
    "                        2,\n",
    "                    )\n",
    "                except ZeroDivisionError:\n",
    "                    portfolioDict[\"sharePriceAtEndOfQtr\"] = float(int(\"0\" + \"\"))\n",
    "\n",
    "                portfolioDict[\"shares_bonds\"] = row.find(\"sshPrnamtType\").text.strip()\n",
    "\n",
    "                if row.find(\"putCall\") is not None:\n",
    "                    portfolioDict[\"putCall\"] = row.find(\"putCall\").text.strip()\n",
    "                else:\n",
    "                    portfolioDict[\"putCall\"] = \"\"\n",
    "\n",
    "                portfolio.append(pd.DataFrame.from_dict([portfolioDict]))\n",
    "\n",
    "            # concatanate secutires\n",
    "            dtPortfolio = pd.concat(portfolio, sort=False, ignore_index=True)\n",
    "        else:\n",
    "            dtPortfolio = pd.DataFrame.from_dict(\n",
    "                [\n",
    "                    {\n",
    "                        \"cusip\": \"\",\n",
    "                        \"nameOfIssuer\": \"\",\n",
    "                        \"titleOfClass\": \"\",\n",
    "                        \"sharesValue\": float(\"0.0\" + \"\"),\n",
    "                        \"sharesHeldAtEndOfQtr\": int(\"0\" + \"\"),\n",
    "                        \"sharePriceAtEndOfQtr\": float(\"0.0\" + \"\"),\n",
    "                        \"shares_bonds\": \"\",\n",
    "                        \"putCall\": \"\",\n",
    "                        \"edgar_path\": file_path,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return dtPortfolio\n",
    "\n",
    "    pages_header, pages_xml, xml_flag, reportType, document1 = work_content_extract(\n",
    "        file_path\n",
    "    )\n",
    "    df1, df2 = parse_institutionalInvestorInfo(\n",
    "        file_path\n",
    "    ), parse_institutionalInvestorPortfolio(file_path)\n",
    "\n",
    "    data = pd.merge(df1, df2, on=\"edgar_path\")\n",
    "    data = data.assign(dsource='sec_app')    \n",
    "    data = data.rename(columns={'accessionNumber': 'accession_number', \"cikManager\": 'cik', \"managerName\":'cik_name','xml_flag':'type',\n",
    "           \"periodOfReport\": 'rdate', \"submissionType\": 'submission_type', \"filedAsOfDate\":'fdate', \"entryTotal\": \"entry_total\",\n",
    "           \"valueTotal\": \"value_total\", 'putCall':'put_call', \"sharesValue\":'value', \"sharesHeldAtEndOfQtr\":'shares',\n",
    "           \"securityType\": \"security_type\", 'titleOfClass':'title_of_class', 'nameOfIssuer':'name_of_issuer' , \"edgar_path\":'file'})\n",
    "\n",
    "    column_names = [\n",
    "        \"accession_number\",\n",
    "        \"cik\",\n",
    "        \"cik_name\",\n",
    "        \"rdate\",\n",
    "        \"submission_type\",\n",
    "        \"fdate\",\n",
    "        \"entry_total\",\n",
    "        \"value_total\",\n",
    "        \"cusip\",\n",
    "        \"name_of_issuer\",\n",
    "        \"title_of_class\",\n",
    "        \"value\",\n",
    "        \"shares\",\n",
    "        \"shares_bonds\",\n",
    "        \"put_call\",\n",
    "        \"type\",\n",
    "        \"created_at\",\n",
    "        \"file\",\n",
    "        'dsource'\n",
    "    ]\n",
    "    data = data.astype(\n",
    "        {\n",
    "            \"accession_number\": str,\n",
    "            \"cik\": \"Int64\",\n",
    "            'cik_name': str,\n",
    "            \"rdate\": \"datetime64[ns]\",\n",
    "            \"submission_type\": str,\n",
    "            \"fdate\": \"datetime64[ns]\",\n",
    "            \"entry_total\": \"Int64\",\n",
    "            \"value_total\": \"float64\",\n",
    "            \"cusip\": str,\n",
    "            \"name_of_issuer\": str,\n",
    "            \"title_of_class\": str,\n",
    "            \"value\": \"float64\",\n",
    "            \"shares\": \"float64\",\n",
    "            \"shares_bonds\": str,\n",
    "            \"put_call\": str,\n",
    "            \"type\": str,\n",
    "            \"created_at\": \"datetime64[ns]\",\n",
    "            \"file\": str,\n",
    "            'dsource': str\n",
    "        }\n",
    "    )\n",
    "    attributes = {\n",
    "        \"accession_number\": \"first\",\n",
    "        \"cik\": \"first\",\n",
    "        \"cik_name\": \"first\",\n",
    "        \"rdate\": \"first\",\n",
    "        \"submission_type\": \"first\",\n",
    "        \"fdate\": \"first\",\n",
    "        \"entry_total\": \"first\",\n",
    "        \"value_total\": \"first\",\n",
    "        \"cusip\": \"first\",\n",
    "        \"name_of_issuer\": \"first\",\n",
    "        \"title_of_class\": \"first\",\n",
    "        \"value\": \"sum\",\n",
    "        \"shares\": \"sum\",\n",
    "        \"shares_bonds\": \"first\",\n",
    "        \"put_call\": \"first\",\n",
    "        \"type\": \"first\",\n",
    "        \"file\": \"first\",\n",
    "        'dsource': 'first'\n",
    "    }\n",
    "    data = data.assign(cusip=data.cusip.str.upper())\n",
    "    data = data.reindex(columns=column_names)\n",
    "   \n",
    "    ## pandera code\n",
    "    validation_schema = pa.DataFrameSchema({\n",
    "    \"cusip\": pa.Column(str,\n",
    "                       pa.Check(lambda s: s.str.len() == 9),\n",
    "                       required=True, nullable=False),\n",
    "    \"value\":  pa.Column(float, pa.Check(lambda s: s != 0.0),required=True, nullable=False),\n",
    "    \"shares\": pa.Column(float, pa.Check(lambda s: s != 0.0), required=True, nullable=False)\n",
    "        })\n",
    "    if data.head(1).type.squeeze() != 'NO':\n",
    "        try:\n",
    "            validation_schema.validate(data, lazy=True)\n",
    "            if not data.empty:\n",
    "                # group cusips\n",
    "                df2 = data.groupby([\"cusip\"], as_index=False).agg(attributes)\n",
    "                df2.to_parquet(\n",
    "                    os.path.join(\n",
    "                        directory_parquet,\n",
    "                        f\"{df2.head(1).cik[0]}-{df2.head(1).accession_number[0]}-{df2.head(1).fdate[0].strftime('%Y-%m-%d')}.parquet\",\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                # return df2\n",
    "            else: pd.DataFrame()\n",
    "\n",
    "        except pa.errors.SchemaErrors as e:\n",
    "            \n",
    "            failure_cases = e.failure_cases\n",
    "            \n",
    "            failure_cases = (failure_cases.assign(df_file=data.file,\n",
    "                                                 df_cik=data.cik,\n",
    "                                                 df_rdate=data.rdate,\n",
    "                                                 df_fdate=data.fdate,\n",
    "                                                 df_value=data.value)).astype({'failure_case':str})\n",
    "\n",
    "            \n",
    "            # return failure_cases\n",
    "            failure_cases.to_parquet(Path.joinpath(failures_parq_dir, \\\n",
    "                                                   f\"bad-{data.head(1).cik[0]}-{file.stem}-{data.head(1).fdate[0].strftime('%Y-%m-%d')}.parquet\"))\n",
    "\n",
    "            data = data[~data.index.isin(failure_cases[\"index\"])]\n",
    "            if not data.empty:\n",
    "                # group cusips\n",
    "                df2 = data.groupby([\"cusip\"], as_index=False).agg(attributes)\n",
    "                df2.to_parquet(\n",
    "                    os.path.join(\n",
    "                        directory_parquet,\n",
    "                        f\"{df2.head(1).cik[0]}-{df2.head(1).accession_number[0]}-{df2.head(1).fdate[0].strftime('%Y-%m-%d')}.parquet\",\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                # return df2\n",
    "            else: pd.DataFrame()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c63c78-8e51-48da-b05b-cce99d46766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read list of csvs with variable number of columns\n",
    "# we define what columns we need \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# txt = \"\"\"AB,CD,EF,GH\n",
    "# foo,20160101,a,1\n",
    "# foo,20160102,a,3\n",
    "# foo,20160103,a,5\"\"\"\n",
    "\n",
    "txt2 = \"\"\"AB,CD,EF\n",
    "foo,20160101,1\n",
    "foo,20160102,1\n",
    "foo,20160103,1\"\"\"\n",
    "\n",
    "# usecols = ['AB', 'CD', 'EF', 'GH','IJ']\n",
    "\n",
    "# df = pd.read_csv(StringIO(txt2), usecols=lambda c: c in set(usecols))\n",
    "df = pd.read_csv(StringIO(txt2))\n",
    "df.EF.head(1).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af6ae9-aa9b-4807-a6b3-aeb76843d123",
   "metadata": {},
   "source": [
    "#### A very interesting use of lambda, dictionary comprehension creacion from Andrej Karpathy's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "36cbc8ed-1c04-420e-92de-1be7cc20eed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.ascii_letters\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(string.ascii_letters+' ')}\n",
    "itos = {i:ch for i, ch in enumerate(string.ascii_letters+' ')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f3efe9e9-e093-4bb1-bc8b-2bfa3e640a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hii itheerre'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "encode('hii itheerre')\n",
    "decode(encode('hii itheerre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423bda3-9666-4019-8a9d-94f10fb4f9a7",
   "metadata": {},
   "source": [
    "#### Let's practice a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c53a1363-46e8-49ba-8905-8a8d9c68b206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lambda func to list all files in a given dir\n",
    "from pathlib import Path\n",
    "sec_app_parq = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_ALL_PARQ_SEC_APP')\n",
    "\n",
    "# list_files_in_dir = lambda dir_: sorted(list(Path(dir_).glob('*'))) \n",
    "# list_cik_in_dir = lambda dir_: sorted(set([int(file.stem.split('-')[0]) for file in Path(dir_).glob('*')]))\n",
    "list_fdates_in_dir = lambda dir_: sorted(set([file.stem.split('-', 4)[-1] for file in Path(dir_).glob('*')]))\n",
    "# list_cik_after = lambda dir_, cik_after: [cik for cik in list_cik_in_dir(dir_) if cik > cik_after ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4bdc9d5d-737d-4d89-afd8-b345b9de406f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 531 ms, sys: 207 ms, total: 738 ms\n",
      "Wall time: 753 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2013-05-20', '2023-02-17')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# list_files(sec_app_parq)[:4]\n",
    "# list_cik_in_dir(sec_app_parq)[:4]\n",
    "# list_fdates_in_dir(sec_app_parq)[:4]\n",
    "\n",
    "min(list_fdates_in_dir(sec_app_parq)), max(list_fdates_in_dir(sec_app_parq))\n",
    "\n",
    "# print(itemgetter(0, -1)(list_cik_after(sec_app_parq, 1958491)))\n",
    "\n",
    "# list_cik_in_dir = lambda dir_: sorted(set([int(file.stem.split('-')[0]) for file in Path(dir_).glob('*')]))\n",
    "# list_cik_after = lambda dir_, cik_after: [cik for cik in list_cik_in_dir(dir_) if cik > cik_after ]\n",
    "# list_cik_after(sec_app_parq, 1958491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c878240a-ff4b-40d6-bf7b-9755e40bed26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "914b1450-77a0-4846-a3de-1a9a5f5379e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getattr(object, 'y', 5)\n",
    "# getattr(object, 'x') is completely equivalent to object.x , but getattr(object, 'y', 5), sets a defaut value and won't error out if 'y' doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "885691ae-a291-4b27-b514-08b83bc71ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2013-05-21', '2013-05-22')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "l = ['2013-05-20', '2013-05-21', '2013-05-22', '2013-05-23', '2017-05-23']\n",
    "\n",
    "itemgetter(1, 2)(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d9dd8903-fe08-40e3-b5ef-47f903dd0750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "falseValue = 345\n",
    "trueValue  = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "90647cda-9c8a-4252-82b2-6f97f5fa445b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(falseValue, trueValue)[bool(True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "90173255-781f-4585-b9fb-eabbd846981a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "353d8af8-04ad-4af8-a7eb-0519b1cb976f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was right\n"
     ]
    }
   ],
   "source": [
    "(lambda: print('I was wrong'), lambda: print('I was right'))[bool(True)]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "91eec6b5-0792-4f06-9342-d6828e3a0d72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was right\n"
     ]
    }
   ],
   "source": [
    "x = print('I was right') if True else rint('I was wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6ae184a0-189b-4df3-abe7-c49377475620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropbox\n",
      "dropbox\n",
      "dropbox\n",
      "dropbox\n",
      "sec_app\n"
     ]
    }
   ],
   "source": [
    "# ternary operator\n",
    "for date in l:\n",
    "    x = 'dropbox' if date <= '2014-01-01' else 'sec_app'\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5b1b57f0-7000-40e0-b198-99cf6c11048a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way how to launch programs from python\n",
    "import subprocess\n",
    "file = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_ALL_PARQ_SEC_APP/1953154-0001214659-22-013465-2022-11-10.parquet')\n",
    "# subprocess.call([\"ls\", \"-l\"])\n",
    "subprocess.call(['tad', file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba042a-3d66-46c3-866d-dc097175eb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0549f233-bc1b-4b5a-a894-db2cfe7683e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda func to list all files in a given dir\n",
    "from pathlib import Path\n",
    "sec_app_parq = Path(r'/Users/yo_macbook/Documents/app_data/sec_apps_data/TR_02_ALL_PARQ_SEC_APP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d80bfd57-0475-4735-b49c-36d8182f4f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_file_names = lambda _dir: [file.stem for file in Path(_dir).glob('*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "739dc3b9-9baa-4302-8a9b-70387ea26c59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1727862-0001727862-23-000013-2023-02-01',\n",
       " '1825516-0001062993-21-003866-2021-04-28',\n",
       " '1801265-0001801265-22-000006-2022-08-15',\n",
       " '1358253-0000919574-15-006055-2015-08-14']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_file_names(sec_app_parq)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7ad56904-b338-40ee-b77a-0a15050448ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "last_date = lambda _dir: [file.stem.split('-', 4)[-1] for file in Path(_dir).glob('*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "55ba6882-fa4e-4b00-9fb7-ba634175dbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2013-05-20', '2023-02-17')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(last_date(sec_app_parq)), max(last_date(sec_app_parq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03edc7b3-a6e9-4c70-9ccf-a533caa59356",
   "metadata": {},
   "source": [
    "#### Lambda with two parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "63897d4e-a388-4bc4-ba9c-f19d2747e612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_parq_in_dir = lambda folder, ext: [file.stem for file in Path(folder).glob('*') if file.suffix == ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b099d9f4-3179-4915-91e7-958400bc0767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_parq_in_dir(sec_app_parq, '.csv')[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "30b646a4-2394-4a91-8644-f6b1704cea00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'865623-0000922423-01-500513'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'865623-0000922423-01-500513-2001-07-16'.rsplit('-', 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cda9231f-83bf-4049-b6b1-fd032b7507f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "list_file_names_csv = lambda folder: [file.stem for file in folder.glob('*.csv')]\n",
    "list_file_names_parq = lambda folder: [file.stem.rsplit('-', 3)[0] for file in folder.glob('*.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fce8c0e3-f345-43ec-8f80-4c3891f9a03c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_csv_0_2013 = Path(r\"/Users/yo_macbook/Documents/app_data/dropbox_13f_files/processed_tables/TR_01_ALL_CSV_CLEANED\")\n",
    "clean_parq_0_2013 = Path(r'/Users/yo_macbook/Documents/app_data/dropbox_13f_files/processed_tables/TR_02_CLEANED_PARQ_0_2013')\n",
    "\n",
    "csv_files = list_file_names_csv(clean_csv_0_2013)\n",
    "parquet_files = list_file_names_parq(clean_parq_0_2013)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaf8da-e8e7-4bbf-9e0b-1a15324b0ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(csv_files), len(parquet_files) \n",
    "csv_files[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a4c69-e68b-4be8-b465-d3dab685d74f",
   "metadata": {},
   "source": [
    "### Find missing parquet files. It's difference between csv and parquet lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "eb918d72-02c6-4b48-b7e7-2c27e8bc703c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l = np.setdiff1d(csv_files, parquet_files)\n",
    "l_reverse = np.setdiff1d(parquet_files, csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b4f7430e-3337-41e9-be76-de24254f2fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1215613-0000905718-03-000066\n",
      "872573-0000872573-03-000005\n",
      "948046-0001092307-02-000098\n",
      "948046-0001092307-02-000110\n",
      "948046-0001092307-02-000112\n",
      "948046-0001092307-02-000113\n",
      "948046-0001092307-02-000128\n",
      "948046-0001092307-02-000129\n",
      "948046-0001092307-03-000132\n",
      "948046-0001092307-03-000152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(name) for name in l if name not in parquet_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cbf2f61d-1e33-459a-9a8e-bbf4bc552375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path.joinpath(clean_csv_0_2013, '1215613-0000905718-03-000066.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e201e8b1-bd3e-49d4-b294-98c5593ebb16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>cusip9</th>\n",
       "      <th>value</th>\n",
       "      <th>shares</th>\n",
       "      <th>rdate</th>\n",
       "      <th>fdate</th>\n",
       "      <th>address</th>\n",
       "      <th>form</th>\n",
       "      <th>shrsOrPrnAmt</th>\n",
       "      <th>putCall</th>\n",
       "      <th>nameOfIssuer</th>\n",
       "      <th>titleOfClass</th>\n",
       "      <th>type</th>\n",
       "      <th>dsource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cik, cusip9, value, shares, rdate, fdate, address, form, shrsOrPrnAmt, putCall, nameOfIssuer, titleOfClass, type, dsource]\n",
       "Index: []"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
